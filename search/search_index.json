{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pydebiaseddta The API documentation for pydebiaseddta , a python library to improve the generalizability of drug-target affinity (DTA) prediction models. The documentation on this website is continuously updated to further ease the use of DebiasedDTA. Installation Instructions conda create --name pydebiaseddta python=3.9.7 conda activate pydebiaseddta python3 -m pip install pydebiaseddta Citing If you use pydebiaseddta in your research, please cite: @article{ozccelik2022debiaseddta, title={DebiasedDTA: DebiasedDTA: Improving the Generalizability of Drug-Target Affinity Prediction Models}, author={{\\\"O}z{\\c{c}}elik, R{\\i}za and Ba{\\u{g}}, Alperen and At{\\i}l, Berk and Barsbey, Melih and {\\\"O}zg{\\\"u}r, Arzucan and {\\\"O}zk{\\i}r{\\i}ml{\\i}, Elif}, journal={arXiv preprint arXiv:2107.05556}, year={2022} }","title":"Homepage"},{"location":"#pydebiaseddta","text":"The API documentation for pydebiaseddta , a python library to improve the generalizability of drug-target affinity (DTA) prediction models. The documentation on this website is continuously updated to further ease the use of DebiasedDTA.","title":"pydebiaseddta"},{"location":"#installation-instructions","text":"conda create --name pydebiaseddta python=3.9.7 conda activate pydebiaseddta python3 -m pip install pydebiaseddta","title":"Installation Instructions"},{"location":"#citing","text":"If you use pydebiaseddta in your research, please cite: @article{ozccelik2022debiaseddta, title={DebiasedDTA: DebiasedDTA: Improving the Generalizability of Drug-Target Affinity Prediction Models}, author={{\\\"O}z{\\c{c}}elik, R{\\i}za and Ba{\\u{g}}, Alperen and At{\\i}l, Berk and Barsbey, Melih and {\\\"O}zg{\\\"u}r, Arzucan and {\\\"O}zk{\\i}r{\\i}ml{\\i}, Elif}, journal={arXiv preprint arXiv:2107.05556}, year={2022} }","title":"Citing"},{"location":"api/debiasing/","text":"debiasing The module that contains the DebiasedDTA training framework. DebiasedDTA training framework comprises two-stages, which we call the \u201cguide\u201d and \u201cpredictor\u201d models. The guide learns a weighting of the training dataset such that a model trained thereupon can learn a robust relationship between biomolecules and binding affinity, instead of spurious associations. The predictor then uses the weights produced by the guide to progressively weight the training data during its training, in order to obtain a predictor that can generalize well to unseen molecules. DebiasedDTA leverages the guides to identify protein-ligand pairs that bear more information about the mechanisms of protein-ligand binding. We hypothesize that, if the guides, models designed to learn misleading spurious patterns, perform poorly on a protein-ligand pair, then the pair is more likely to bear generalizable information on binding and deserves higher attention by the DTA predictors. DebiasedDTA adopts k-fold cross-validation ( k= 1 / mini_val_frac ) to measure the performance of a guide on the training interactions. First, it randomly divide the training set into five folds and construct five different mini-training and mini-validation sets. DebiasedDTA trains the guide on each mini-training set and compute the squared errors on the corresponding mini-validation set. One run of cross-validation yields one squared-error measurement per protein-ligand pair as each pair is placed in the mini-validation set exactly once. In order to better estimate the performance on each sample, DebiasedDTA runs the cross-validation n_bootstrapping times and obtains n_bootstrapping error measurements per sample. DebiasedDTA computes the median of the n_bootstrapping squared errors and calls it the \"importance coefficient\" of a protein-ligand pair. The importance coefficients guide the training of the predictor after being converted into training weights. In the DebiasedDTA training framework, the predictor is the model that will be trained with the training samples weighted by the guide to ultimately predict target protein-chemical affinities. The predictor can adopt any biomolecule representation, but has to be able to weight the training samples during training to comply with the weight adaptation strategy proposed in DebiasedDTA. The proposed strategy initializes the training sample weights to 1 and updates them at each epoch such that the weight of each training sample converges to its importance coefficient at the last epoch. When trained with this strategy, the predictor attributes higher importance to samples with more information on binding rules ( i.e. samples with higher importance coefficient) as the learning continues. Our weight adaptation strategy is formulated as \\[\\vec{w}_e = (1 - \\frac{e}{E}) + \\vec{i} \\times \\frac{e}{E}, \\] where \\(\\vec{w}_e\\) is the vector of training sample weights at epoch \\(e\\) , \\(E\\) is the number of training epochs, and \\(\\vec{i}\\) is the importance coefficients vector. Here, \\(e/E\\) increases as the training continues, and so does the impact of \\(\\vec{i}\\) , importance coefficients, on the sample weights. DebiasedDTA Source code in pydebiaseddta\\debiasing\\debiaseddta.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class DebiasedDTA : def __init__ ( self , guide_cls : Type [ guides . Guide ], predictor_cls : Type [ predictors . BasePredictor ], mini_val_frac : float = 0.2 , n_bootstrapping : int = 10 , guide_params : Dict = None , predictor_params : Dict = None , ): \"\"\"Constructor to initiate a DebiasedDTA training framework. Parameters ---------- guide_cls : Type[guides.AbstractGuide] The `Guide` class for debiasing. Note that the input is not an instance, but a class, *e.g.*, `BoWDTA`, not `BoWDTA()`. The instance is created during the model training by the DebiasedDTA module. predictor_cls : Type[predictors.BasePredictor] Class of the `Predictor` to debias. Note that the input is not an instance, but a class, *e.g.*, `BPEDTA`, not `BPEDTA()`. The instance is created during the model training by the DebiasedDTA module. mini_val_frac : float, optional Fraction of the validation data to separate for guide evaluation, by default 0.2 n_bootstrapping : int, optional Number of times to train guides on the training set, by default 10 guide_params : Dict, optional Parameter dictionary necessary to create the `Guide`. The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. predictor_params : Dict, optional Parameter dictionary necessary to create the `Predictor`. The dictionary should map the name of the constructor parameters to their values, and `n_epochs` **must** be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. Raises ------ ValueError A `ValueError` is raised if `n_epochs` is not among the predictor parameters. \"\"\" self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = dict () if guide_params is None else guide_params self . predictor_params = {} if predictor_params is None else predictor_params self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' ) @staticmethod def save_importance_coefficients ( interactions : List [ Tuple [ int , Any , Any , float ]], importance_coefficients : List [ float ], savedir : str ): \"\"\"Saves the importance coefficients learned by the `guide`. Parameters ---------- interactions : List[Tuple[int, Any, Any, float]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. importance_coefficients : List[float] The importance coefficient for each interaction. savedir : str Path to save the coefficients. \"\"\" dump_content = [] for interaction_id , ligand , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { ligand } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump ) def learn_importance_coefficients ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], savedir : str = None , ) -> List [ float ]: \"\"\"Learns importance coefficients using the `Guide` model specified during the construction. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training proteins used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. savedir : str, optional Path to save the learned importance coefficients. By default `None` and the coefficients are not saved. Returns ------- List[float] The importance coefficients learned by the guide. \"\"\" train_size = len ( train_ligands ) train_interactions = list ( zip ( range ( train_size ), train_ligands , train_proteins , train_labels ,) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for _ in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) mini_train_ligands = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_ligands , mini_train_proteins , mini_train_labels , ) mini_val_ligands = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_ligands , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds )) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , coeffs_save_path : str = None , ) -> Any : \"\"\"Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[Any], optional Validation ligands to measure predictor performance, by default `None` and no validation is applied. val_proteins : List[Any], optional Validation proteins to measure predictor performance, by default `None` and no validation is applied. val_labels : List[float], optional Affinity scores of the Validatio pairs, by default `None` and no validation is applied. coeffs_save_path : str, optional Path to save importance coefficients learned by the `guide`. Defaults to `None` and no saving is performed. Returns ------- Any Output of the train function of the predictor. \"\"\" train_ligands = train_ligands . copy () train_proteins = train_proteins . copy () importance_coefficients = self . learn_importance_coefficients ( train_ligands , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , val_ligands = val_ligands , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , ) __init__ ( guide_cls , predictor_cls , mini_val_frac = 0.2 , n_bootstrapping = 10 , guide_params = None , predictor_params = None ) Constructor to initiate a DebiasedDTA training framework. Parameters: Name Type Description Default guide_cls Type [ guides . AbstractGuide ] The Guide class for debiasing. Note that the input is not an instance, but a class, e.g. , BoWDTA , not BoWDTA() . The instance is created during the model training by the DebiasedDTA module. required predictor_cls Type [ predictors . BasePredictor ] Class of the Predictor to debias. Note that the input is not an instance, but a class, e.g. , BPEDTA , not BPEDTA() . The instance is created during the model training by the DebiasedDTA module. required mini_val_frac float , optional Fraction of the validation data to separate for guide evaluation, by default 0.2 0.2 n_bootstrapping int , optional Number of times to train guides on the training set, by default 10 10 guide_params Dict , optional Parameter dictionary necessary to create the Guide . The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. None predictor_params Dict , optional Parameter dictionary necessary to create the Predictor . The dictionary should map the name of the constructor parameters to their values, and n_epochs must be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. None Raises: Type Description ValueError A ValueError is raised if n_epochs is not among the predictor parameters. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , guide_cls : Type [ guides . Guide ], predictor_cls : Type [ predictors . BasePredictor ], mini_val_frac : float = 0.2 , n_bootstrapping : int = 10 , guide_params : Dict = None , predictor_params : Dict = None , ): \"\"\"Constructor to initiate a DebiasedDTA training framework. Parameters ---------- guide_cls : Type[guides.AbstractGuide] The `Guide` class for debiasing. Note that the input is not an instance, but a class, *e.g.*, `BoWDTA`, not `BoWDTA()`. The instance is created during the model training by the DebiasedDTA module. predictor_cls : Type[predictors.BasePredictor] Class of the `Predictor` to debias. Note that the input is not an instance, but a class, *e.g.*, `BPEDTA`, not `BPEDTA()`. The instance is created during the model training by the DebiasedDTA module. mini_val_frac : float, optional Fraction of the validation data to separate for guide evaluation, by default 0.2 n_bootstrapping : int, optional Number of times to train guides on the training set, by default 10 guide_params : Dict, optional Parameter dictionary necessary to create the `Guide`. The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. predictor_params : Dict, optional Parameter dictionary necessary to create the `Predictor`. The dictionary should map the name of the constructor parameters to their values, and `n_epochs` **must** be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. Raises ------ ValueError A `ValueError` is raised if `n_epochs` is not among the predictor parameters. \"\"\" self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = dict () if guide_params is None else guide_params self . predictor_params = {} if predictor_params is None else predictor_params self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' ) learn_importance_coefficients ( train_ligands , train_proteins , train_labels , savedir = None ) Learns importance coefficients using the Guide model specified during the construction. Parameters: Name Type Description Default train_ligands List [ Any ] List of the training ligands used by the Guide model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. required train_proteins List [ Any ] List of the training proteins used by the Guide model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required savedir str , optional Path to save the learned importance coefficients. By default None and the coefficients are not saved. None Returns: Type Description List [ float ] The importance coefficients learned by the guide. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def learn_importance_coefficients ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], savedir : str = None , ) -> List [ float ]: \"\"\"Learns importance coefficients using the `Guide` model specified during the construction. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training proteins used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. savedir : str, optional Path to save the learned importance coefficients. By default `None` and the coefficients are not saved. Returns ------- List[float] The importance coefficients learned by the guide. \"\"\" train_size = len ( train_ligands ) train_interactions = list ( zip ( range ( train_size ), train_ligands , train_proteins , train_labels ,) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for _ in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) mini_train_ligands = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_ligands , mini_train_proteins , mini_train_labels , ) mini_val_ligands = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_ligands , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds )) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients save_importance_coefficients ( interactions , importance_coefficients , savedir ) staticmethod Saves the importance coefficients learned by the guide . Parameters: Name Type Description Default interactions List [ Tuple [ int , Any , Any , float ]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. required importance_coefficients List [ float ] The importance coefficient for each interaction. required savedir str Path to save the coefficients. required Source code in pydebiaseddta\\debiasing\\debiaseddta.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @staticmethod def save_importance_coefficients ( interactions : List [ Tuple [ int , Any , Any , float ]], importance_coefficients : List [ float ], savedir : str ): \"\"\"Saves the importance coefficients learned by the `guide`. Parameters ---------- interactions : List[Tuple[int, Any, Any, float]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. importance_coefficients : List[float] The importance coefficient for each interaction. savedir : str Path to save the coefficients. \"\"\" dump_content = [] for interaction_id , ligand , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { ligand } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump ) train ( train_ligands , train_proteins , train_labels , val_ligands = None , val_proteins = None , val_labels = None , coeffs_save_path = None ) Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters: Name Type Description Default train_ligands List [ Any ] List of the training ligands used by the Predictor . DebiasedDTA training framework imposes no restriction on the representation type of the ligands. required train_proteins List [ Any ] List of the training ligands used by the Predictor . DebiasedDTA training framework imposes no restriction on the representation type of the proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ Any ], optional Validation ligands to measure predictor performance, by default None and no validation is applied. None val_proteins List [ Any ], optional Validation proteins to measure predictor performance, by default None and no validation is applied. None val_labels List [ float ], optional Affinity scores of the Validatio pairs, by default None and no validation is applied. None coeffs_save_path str , optional Path to save importance coefficients learned by the guide . Defaults to None and no saving is performed. None Returns: Type Description Any Output of the train function of the predictor. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , coeffs_save_path : str = None , ) -> Any : \"\"\"Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[Any], optional Validation ligands to measure predictor performance, by default `None` and no validation is applied. val_proteins : List[Any], optional Validation proteins to measure predictor performance, by default `None` and no validation is applied. val_labels : List[float], optional Affinity scores of the Validatio pairs, by default `None` and no validation is applied. coeffs_save_path : str, optional Path to save importance coefficients learned by the `guide`. Defaults to `None` and no saving is performed. Returns ------- Any Output of the train function of the predictor. \"\"\" train_ligands = train_ligands . copy () train_proteins = train_proteins . copy () importance_coefficients = self . learn_importance_coefficients ( train_ligands , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , val_ligands = val_ligands , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"debiasing"},{"location":"api/debiasing/#debiasing","text":"The module that contains the DebiasedDTA training framework. DebiasedDTA training framework comprises two-stages, which we call the \u201cguide\u201d and \u201cpredictor\u201d models. The guide learns a weighting of the training dataset such that a model trained thereupon can learn a robust relationship between biomolecules and binding affinity, instead of spurious associations. The predictor then uses the weights produced by the guide to progressively weight the training data during its training, in order to obtain a predictor that can generalize well to unseen molecules. DebiasedDTA leverages the guides to identify protein-ligand pairs that bear more information about the mechanisms of protein-ligand binding. We hypothesize that, if the guides, models designed to learn misleading spurious patterns, perform poorly on a protein-ligand pair, then the pair is more likely to bear generalizable information on binding and deserves higher attention by the DTA predictors. DebiasedDTA adopts k-fold cross-validation ( k= 1 / mini_val_frac ) to measure the performance of a guide on the training interactions. First, it randomly divide the training set into five folds and construct five different mini-training and mini-validation sets. DebiasedDTA trains the guide on each mini-training set and compute the squared errors on the corresponding mini-validation set. One run of cross-validation yields one squared-error measurement per protein-ligand pair as each pair is placed in the mini-validation set exactly once. In order to better estimate the performance on each sample, DebiasedDTA runs the cross-validation n_bootstrapping times and obtains n_bootstrapping error measurements per sample. DebiasedDTA computes the median of the n_bootstrapping squared errors and calls it the \"importance coefficient\" of a protein-ligand pair. The importance coefficients guide the training of the predictor after being converted into training weights. In the DebiasedDTA training framework, the predictor is the model that will be trained with the training samples weighted by the guide to ultimately predict target protein-chemical affinities. The predictor can adopt any biomolecule representation, but has to be able to weight the training samples during training to comply with the weight adaptation strategy proposed in DebiasedDTA. The proposed strategy initializes the training sample weights to 1 and updates them at each epoch such that the weight of each training sample converges to its importance coefficient at the last epoch. When trained with this strategy, the predictor attributes higher importance to samples with more information on binding rules ( i.e. samples with higher importance coefficient) as the learning continues. Our weight adaptation strategy is formulated as \\[\\vec{w}_e = (1 - \\frac{e}{E}) + \\vec{i} \\times \\frac{e}{E}, \\] where \\(\\vec{w}_e\\) is the vector of training sample weights at epoch \\(e\\) , \\(E\\) is the number of training epochs, and \\(\\vec{i}\\) is the importance coefficients vector. Here, \\(e/E\\) increases as the training continues, and so does the impact of \\(\\vec{i}\\) , importance coefficients, on the sample weights.","title":"debiasing"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA","text":"Source code in pydebiaseddta\\debiasing\\debiaseddta.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class DebiasedDTA : def __init__ ( self , guide_cls : Type [ guides . Guide ], predictor_cls : Type [ predictors . BasePredictor ], mini_val_frac : float = 0.2 , n_bootstrapping : int = 10 , guide_params : Dict = None , predictor_params : Dict = None , ): \"\"\"Constructor to initiate a DebiasedDTA training framework. Parameters ---------- guide_cls : Type[guides.AbstractGuide] The `Guide` class for debiasing. Note that the input is not an instance, but a class, *e.g.*, `BoWDTA`, not `BoWDTA()`. The instance is created during the model training by the DebiasedDTA module. predictor_cls : Type[predictors.BasePredictor] Class of the `Predictor` to debias. Note that the input is not an instance, but a class, *e.g.*, `BPEDTA`, not `BPEDTA()`. The instance is created during the model training by the DebiasedDTA module. mini_val_frac : float, optional Fraction of the validation data to separate for guide evaluation, by default 0.2 n_bootstrapping : int, optional Number of times to train guides on the training set, by default 10 guide_params : Dict, optional Parameter dictionary necessary to create the `Guide`. The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. predictor_params : Dict, optional Parameter dictionary necessary to create the `Predictor`. The dictionary should map the name of the constructor parameters to their values, and `n_epochs` **must** be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. Raises ------ ValueError A `ValueError` is raised if `n_epochs` is not among the predictor parameters. \"\"\" self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = dict () if guide_params is None else guide_params self . predictor_params = {} if predictor_params is None else predictor_params self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' ) @staticmethod def save_importance_coefficients ( interactions : List [ Tuple [ int , Any , Any , float ]], importance_coefficients : List [ float ], savedir : str ): \"\"\"Saves the importance coefficients learned by the `guide`. Parameters ---------- interactions : List[Tuple[int, Any, Any, float]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. importance_coefficients : List[float] The importance coefficient for each interaction. savedir : str Path to save the coefficients. \"\"\" dump_content = [] for interaction_id , ligand , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { ligand } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump ) def learn_importance_coefficients ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], savedir : str = None , ) -> List [ float ]: \"\"\"Learns importance coefficients using the `Guide` model specified during the construction. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training proteins used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. savedir : str, optional Path to save the learned importance coefficients. By default `None` and the coefficients are not saved. Returns ------- List[float] The importance coefficients learned by the guide. \"\"\" train_size = len ( train_ligands ) train_interactions = list ( zip ( range ( train_size ), train_ligands , train_proteins , train_labels ,) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for _ in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) mini_train_ligands = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_ligands , mini_train_proteins , mini_train_labels , ) mini_val_ligands = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_ligands , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds )) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , coeffs_save_path : str = None , ) -> Any : \"\"\"Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[Any], optional Validation ligands to measure predictor performance, by default `None` and no validation is applied. val_proteins : List[Any], optional Validation proteins to measure predictor performance, by default `None` and no validation is applied. val_labels : List[float], optional Affinity scores of the Validatio pairs, by default `None` and no validation is applied. coeffs_save_path : str, optional Path to save importance coefficients learned by the `guide`. Defaults to `None` and no saving is performed. Returns ------- Any Output of the train function of the predictor. \"\"\" train_ligands = train_ligands . copy () train_proteins = train_proteins . copy () importance_coefficients = self . learn_importance_coefficients ( train_ligands , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , val_ligands = val_ligands , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"DebiasedDTA"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA.__init__","text":"Constructor to initiate a DebiasedDTA training framework. Parameters: Name Type Description Default guide_cls Type [ guides . AbstractGuide ] The Guide class for debiasing. Note that the input is not an instance, but a class, e.g. , BoWDTA , not BoWDTA() . The instance is created during the model training by the DebiasedDTA module. required predictor_cls Type [ predictors . BasePredictor ] Class of the Predictor to debias. Note that the input is not an instance, but a class, e.g. , BPEDTA , not BPEDTA() . The instance is created during the model training by the DebiasedDTA module. required mini_val_frac float , optional Fraction of the validation data to separate for guide evaluation, by default 0.2 0.2 n_bootstrapping int , optional Number of times to train guides on the training set, by default 10 10 guide_params Dict , optional Parameter dictionary necessary to create the Guide . The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. None predictor_params Dict , optional Parameter dictionary necessary to create the Predictor . The dictionary should map the name of the constructor parameters to their values, and n_epochs must be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. None Raises: Type Description ValueError A ValueError is raised if n_epochs is not among the predictor parameters. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , guide_cls : Type [ guides . Guide ], predictor_cls : Type [ predictors . BasePredictor ], mini_val_frac : float = 0.2 , n_bootstrapping : int = 10 , guide_params : Dict = None , predictor_params : Dict = None , ): \"\"\"Constructor to initiate a DebiasedDTA training framework. Parameters ---------- guide_cls : Type[guides.AbstractGuide] The `Guide` class for debiasing. Note that the input is not an instance, but a class, *e.g.*, `BoWDTA`, not `BoWDTA()`. The instance is created during the model training by the DebiasedDTA module. predictor_cls : Type[predictors.BasePredictor] Class of the `Predictor` to debias. Note that the input is not an instance, but a class, *e.g.*, `BPEDTA`, not `BPEDTA()`. The instance is created during the model training by the DebiasedDTA module. mini_val_frac : float, optional Fraction of the validation data to separate for guide evaluation, by default 0.2 n_bootstrapping : int, optional Number of times to train guides on the training set, by default 10 guide_params : Dict, optional Parameter dictionary necessary to create the `Guide`. The dictionary should map the name of the constructor parameters to their values. An empty dictionary is used during the creation by default. predictor_params : Dict, optional Parameter dictionary necessary to create the `Predictor`. The dictionary should map the name of the constructor parameters to their values, and `n_epochs` **must** be among the parameters for debiasing to work. An empty dictionary is used during the creation by default. Raises ------ ValueError A `ValueError` is raised if `n_epochs` is not among the predictor parameters. \"\"\" self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = dict () if guide_params is None else guide_params self . predictor_params = {} if predictor_params is None else predictor_params self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' )","title":"__init__()"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA.learn_importance_coefficients","text":"Learns importance coefficients using the Guide model specified during the construction. Parameters: Name Type Description Default train_ligands List [ Any ] List of the training ligands used by the Guide model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. required train_proteins List [ Any ] List of the training proteins used by the Guide model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required savedir str , optional Path to save the learned importance coefficients. By default None and the coefficients are not saved. None Returns: Type Description List [ float ] The importance coefficients learned by the guide. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def learn_importance_coefficients ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], savedir : str = None , ) -> List [ float ]: \"\"\"Learns importance coefficients using the `Guide` model specified during the construction. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training proteins used by the `Guide` model. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. savedir : str, optional Path to save the learned importance coefficients. By default `None` and the coefficients are not saved. Returns ------- List[float] The importance coefficients learned by the guide. \"\"\" train_size = len ( train_ligands ) train_interactions = list ( zip ( range ( train_size ), train_ligands , train_proteins , train_labels ,) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for _ in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) mini_train_ligands = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_ligands , mini_train_proteins , mini_train_labels , ) mini_val_ligands = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_ligands , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds )) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients","title":"learn_importance_coefficients()"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA.save_importance_coefficients","text":"Saves the importance coefficients learned by the guide . Parameters: Name Type Description Default interactions List [ Tuple [ int , Any , Any , float ]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. required importance_coefficients List [ float ] The importance coefficient for each interaction. required savedir str Path to save the coefficients. required Source code in pydebiaseddta\\debiasing\\debiaseddta.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @staticmethod def save_importance_coefficients ( interactions : List [ Tuple [ int , Any , Any , float ]], importance_coefficients : List [ float ], savedir : str ): \"\"\"Saves the importance coefficients learned by the `guide`. Parameters ---------- interactions : List[Tuple[int, Any, Any, float]] The List of training interactions as a Tuple of interaction id (assigned by the guide), ligand, chemical, and affinity score. importance_coefficients : List[float] The importance coefficient for each interaction. savedir : str Path to save the coefficients. \"\"\" dump_content = [] for interaction_id , ligand , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { ligand } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump )","title":"save_importance_coefficients()"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA.train","text":"Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters: Name Type Description Default train_ligands List [ Any ] List of the training ligands used by the Predictor . DebiasedDTA training framework imposes no restriction on the representation type of the ligands. required train_proteins List [ Any ] List of the training ligands used by the Predictor . DebiasedDTA training framework imposes no restriction on the representation type of the proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ Any ], optional Validation ligands to measure predictor performance, by default None and no validation is applied. None val_proteins List [ Any ], optional Validation proteins to measure predictor performance, by default None and no validation is applied. None val_labels List [ float ], optional Affinity scores of the Validatio pairs, by default None and no validation is applied. None coeffs_save_path str , optional Path to save importance coefficients learned by the guide . Defaults to None and no saving is performed. None Returns: Type Description Any Output of the train function of the predictor. Source code in pydebiaseddta\\debiasing\\debiaseddta.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , coeffs_save_path : str = None , ) -> Any : \"\"\"Starts the DebiasedDTA training framework. The importance coefficients are learned with the guide and used to weight the samples during the predictor's training. Performance on the validation set is also measured, if provided. Parameters ---------- train_ligands : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the ligands. train_proteins : List[Any] List of the training ligands used by the `Predictor`. DebiasedDTA training framework imposes no restriction on the representation type of the proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[Any], optional Validation ligands to measure predictor performance, by default `None` and no validation is applied. val_proteins : List[Any], optional Validation proteins to measure predictor performance, by default `None` and no validation is applied. val_labels : List[float], optional Affinity scores of the Validatio pairs, by default `None` and no validation is applied. coeffs_save_path : str, optional Path to save importance coefficients learned by the `guide`. Defaults to `None` and no saving is performed. Returns ------- Any Output of the train function of the predictor. \"\"\" train_ligands = train_ligands . copy () train_proteins = train_proteins . copy () importance_coefficients = self . learn_importance_coefficients ( train_ligands , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , val_ligands = val_ligands , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_ligands , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"train()"},{"location":"api/evaluation/","text":"evaluation ci ( gold_truths , predictions ) Computes concordance index (CI) between the expected values and predictions. See G\u00f6nen and Heller (2005) for the details of the metric. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Concordance index. Source code in pydebiaseddta\\evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ci ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes concordance index (CI) between the expected values and predictions. See [G\u00f6nen and Heller (2005)](https://www.jstor.org/stable/20441249) for the details of the metric. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Concordance index. \"\"\" gold_combs , pred_combs = combinations ( gold_truths , 2 ), combinations ( predictions , 2 ) nominator , denominator = 0 , 0 for ( g1 , g2 ), ( p1 , p2 ) in zip ( gold_combs , pred_combs ): if g2 > g1 : nominator = nominator + 1 * ( p2 > p1 ) + 0.5 * ( p2 == p1 ) denominator = denominator + 1 return float ( nominator / denominator ) evaluate_predictions ( gold_truths , predictions , metrics = None ) Computes multiple metrics with a single call for convenience. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required metrics List [ str ] Name of the evaluation metrics to compute. Possible values are: {\"ci\", \"r2\", \"rmse\", \"mse\"} . All metrics are computed if no value is provided. None Returns: Type Description Dict [ str , float ] A dictionary that maps each metric name to the computed value. Source code in pydebiaseddta\\evaluation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_predictions ( gold_truths : List [ float ], predictions : List [ float ], metrics : List [ str ] = None ) -> Dict [ str , float ]: \"\"\"Computes multiple metrics with a single call for convenience. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. metrics : List[str] Name of the evaluation metrics to compute. Possible values are: `{\"ci\", \"r2\", \"rmse\", \"mse\"}`. All metrics are computed if no value is provided. Returns ------- Dict[str,float] A dictionary that maps each metric name to the computed value. \"\"\" if metrics is None : metrics = [ \"ci\" , \"r2\" , \"rmse\" , \"mse\" ] metrics = [ metric . lower () for metric in metrics ] name_to_fn = { \"ci\" : ci , \"r2\" : r2 , \"rmse\" : rmse , \"mse\" : mse } return { metric : name_to_fn [ metric ]( gold_truths , predictions ) for metric in metrics } mse ( gold_truths , predictions ) Computes mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Mean squared error. Source code in pydebiaseddta\\evaluation.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def mse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = True )) r2 ( gold_truths , predictions ) Compute \\(R^2\\) (coefficient of determinant) between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float \\(R^2\\) (coefficient of determinant) score. Source code in pydebiaseddta\\evaluation.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def r2 ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Compute $R^2$ (coefficient of determinant) between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float $R^2$ (coefficient of determinant) score. \"\"\" return float ( r2_score ( gold_truths , predictions )) rmse ( gold_truths , predictions ) Computes root mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Root mean squared error. Source code in pydebiaseddta\\evaluation.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def rmse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes root mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Root mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = False ))","title":"evaluation"},{"location":"api/evaluation/#evaluation","text":"","title":"evaluation"},{"location":"api/evaluation/#pydebiaseddta.evaluation.ci","text":"Computes concordance index (CI) between the expected values and predictions. See G\u00f6nen and Heller (2005) for the details of the metric. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Concordance index. Source code in pydebiaseddta\\evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ci ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes concordance index (CI) between the expected values and predictions. See [G\u00f6nen and Heller (2005)](https://www.jstor.org/stable/20441249) for the details of the metric. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Concordance index. \"\"\" gold_combs , pred_combs = combinations ( gold_truths , 2 ), combinations ( predictions , 2 ) nominator , denominator = 0 , 0 for ( g1 , g2 ), ( p1 , p2 ) in zip ( gold_combs , pred_combs ): if g2 > g1 : nominator = nominator + 1 * ( p2 > p1 ) + 0.5 * ( p2 == p1 ) denominator = denominator + 1 return float ( nominator / denominator )","title":"ci()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.evaluate_predictions","text":"Computes multiple metrics with a single call for convenience. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required metrics List [ str ] Name of the evaluation metrics to compute. Possible values are: {\"ci\", \"r2\", \"rmse\", \"mse\"} . All metrics are computed if no value is provided. None Returns: Type Description Dict [ str , float ] A dictionary that maps each metric name to the computed value. Source code in pydebiaseddta\\evaluation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_predictions ( gold_truths : List [ float ], predictions : List [ float ], metrics : List [ str ] = None ) -> Dict [ str , float ]: \"\"\"Computes multiple metrics with a single call for convenience. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. metrics : List[str] Name of the evaluation metrics to compute. Possible values are: `{\"ci\", \"r2\", \"rmse\", \"mse\"}`. All metrics are computed if no value is provided. Returns ------- Dict[str,float] A dictionary that maps each metric name to the computed value. \"\"\" if metrics is None : metrics = [ \"ci\" , \"r2\" , \"rmse\" , \"mse\" ] metrics = [ metric . lower () for metric in metrics ] name_to_fn = { \"ci\" : ci , \"r2\" : r2 , \"rmse\" : rmse , \"mse\" : mse } return { metric : name_to_fn [ metric ]( gold_truths , predictions ) for metric in metrics }","title":"evaluate_predictions()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.mse","text":"Computes mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Mean squared error. Source code in pydebiaseddta\\evaluation.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def mse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = True ))","title":"mse()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.r2","text":"Compute \\(R^2\\) (coefficient of determinant) between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float \\(R^2\\) (coefficient of determinant) score. Source code in pydebiaseddta\\evaluation.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def r2 ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Compute $R^2$ (coefficient of determinant) between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float $R^2$ (coefficient of determinant) score. \"\"\" return float ( r2_score ( gold_truths , predictions ))","title":"r2()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.rmse","text":"Computes root mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Root mean squared error. Source code in pydebiaseddta\\evaluation.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def rmse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes root mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Root mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = False ))","title":"rmse()"},{"location":"api/guides/","text":"guides The submodule that contains the guides, i.e. , the weak learners in DebiasedDTA that learn a weighting of the training set to improve generalizability. The implemented guides are IDDTA and BoWDTA, and an abstract classes is also available to quickly implement custom guides. Guide Bases: ABC An abstract class that implements the interface of a guide in pydebiaseddta . The guides are characterized by a train function and a predict function, whose signatures are implemented by this class. Any instance of the Guide class can be trained in the DebiasedDTA training framework, and therefore, Guide can be inherited to design custom guide models. Source code in pydebiaseddta\\guides\\abstract_guide.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class Guide ( ABC ): \"\"\"An abstract class that implements the interface of a guide in `pydebiaseddta`. The guides are characterized by a `train` function and a `predict` function, whose signatures are implemented by this class. Any instance of the `Guide` class can be trained in the `DebiasedDTA` training framework, and therefore, `Guide` can be inherited to design custom guide models. \"\"\" @abstractmethod def train ( train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], ): \"\"\"An abstract method to define the training interface of the guides. Parameters ---------- train_ligands : List[Any] Training ligands in any representation. train_proteins : List[Any] Training proteins in any representation. train_labels : List[float] Affinity scores of the training protein-ligand pairs. \"\"\" pass @abstractmethod def predict ( ligands : List [ Any ], proteins : List [ Any ]) -> List [ float ]: \"\"\"An abstract method to define the prediction interface of the guides. Parameters ---------- ligands : List[Any] Ligands in any representation. proteins : List[Any] Proteins in any representation. Returns ------- List[float] The predicted affinities. \"\"\" pass predict ( ligands , proteins ) abstractmethod An abstract method to define the prediction interface of the guides. Parameters: Name Type Description Default ligands List [ Any ] Ligands in any representation. required proteins List [ Any ] Proteins in any representation. required Returns: Type Description List [ float ] The predicted affinities. Source code in pydebiaseddta\\guides\\abstract_guide.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @abstractmethod def predict ( ligands : List [ Any ], proteins : List [ Any ]) -> List [ float ]: \"\"\"An abstract method to define the prediction interface of the guides. Parameters ---------- ligands : List[Any] Ligands in any representation. proteins : List[Any] Proteins in any representation. Returns ------- List[float] The predicted affinities. \"\"\" pass train ( train_ligands , train_proteins , train_labels ) abstractmethod An abstract method to define the training interface of the guides. Parameters: Name Type Description Default train_ligands List [ Any ] Training ligands in any representation. required train_proteins List [ Any ] Training proteins in any representation. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required Source code in pydebiaseddta\\guides\\abstract_guide.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def train ( train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], ): \"\"\"An abstract method to define the training interface of the guides. Parameters ---------- train_ligands : List[Any] Training ligands in any representation. train_proteins : List[Any] Training proteins in any representation. train_labels : List[float] Affinity scores of the training protein-ligand pairs. \"\"\" pass BoWDTA Bases: Guide Source code in pydebiaseddta\\guides\\bowdta.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class BoWDTA ( Guide ): def __init__ ( self ): \"\"\"Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. \"\"\" self . ligand_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"C\" ) self . protein_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"$\" ) self . prediction_model = DecisionTreeRegressor () def tokenize_ligands ( self , smiles : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters ---------- smiles : List[str] The SMILES strings of the ligands Returns ------- List[List[int]] Label encoded sequences of ligand words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( smiles , smi_to_unichar_encoding ) word_identifier = load_ligand_word_identifier ( vocab_size = 8000 ) return word_identifier . encode_sequences ( unichars , 100 ) def tokenize_proteins ( self , aa_sequences : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- List[List[int]] Label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return word_identifier . encode_sequences ( aa_sequences , 1000 ) def vectorize_ligands ( self , smiles_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the ligands based on their frequency. Parameters ---------- smiles_words : List[List[int]] ligand words of each ligand as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . ligand_bow_vectorizer . texts_to_matrix ( smiles_words , mode = \"freq\" ) def vectorize_proteins ( self , protein_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the proteins based on their frequency. Parameters ---------- protein_words : List[List[int]] Protein words of each protein as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . protein_bow_vectorizer . texts_to_matrix ( protein_words , mode = \"freq\" ) def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training ligands. train_labels : List[float] Affinity scores of the training interactions. \"\"\" tokenized_ligands = self . tokenize_ligands ( train_ligands ) tokenized_proteins = self . tokenize_proteins ( train_proteins ) self . ligand_bow_vectorizer . fit_on_texts ( tokenized_ligands ) self . protein_bow_vectorizer . fit_on_texts ( tokenized_proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) X_train = np . hstack ([ ligand_vectors , protein_vectors ]) self . prediction_model . fit ( X_train , train_labels ) def predict ( self , ligands : List [ str ], proteins : List [ str ] ) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" tokenized_ligands = self . tokenize_ligands ( ligands ) tokenized_proteins = self . tokenize_proteins ( proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) interaction = np . hstack ([ ligand_vectors , protein_vectors ]) return self . prediction_model . predict ( interaction ) . tolist () __init__ () Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. Source code in pydebiaseddta\\guides\\bowdta.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self ): \"\"\"Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. \"\"\" self . ligand_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"C\" ) self . protein_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"$\" ) self . prediction_model = DecisionTreeRegressor () predict ( ligands , proteins ) Predicts the affinities of a list of protein-ligand pairs. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinities. Source code in pydebiaseddta\\guides\\bowdta.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def predict ( self , ligands : List [ str ], proteins : List [ str ] ) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" tokenized_ligands = self . tokenize_ligands ( ligands ) tokenized_proteins = self . tokenize_proteins ( proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) interaction = np . hstack ([ ligand_vectors , protein_vectors ]) return self . prediction_model . predict ( interaction ) . tolist () tokenize_ligands ( smiles ) Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters: Name Type Description Default smiles List [ str ] The SMILES strings of the ligands required Returns: Type Description List [ List [ int ]] Label encoded sequences of ligand words. Source code in pydebiaseddta\\guides\\bowdta.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def tokenize_ligands ( self , smiles : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters ---------- smiles : List[str] The SMILES strings of the ligands Returns ------- List[List[int]] Label encoded sequences of ligand words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( smiles , smi_to_unichar_encoding ) word_identifier = load_ligand_word_identifier ( vocab_size = 8000 ) return word_identifier . encode_sequences ( unichars , 100 ) tokenize_proteins ( aa_sequences ) Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of the proteins. required Returns: Type Description List [ List [ int ]] Label encoded sequences of protein words. Source code in pydebiaseddta\\guides\\bowdta.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def tokenize_proteins ( self , aa_sequences : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- List[List[int]] Label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return word_identifier . encode_sequences ( aa_sequences , 1000 ) train ( train_ligands , train_proteins , train_labels ) Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training ligands. required train_labels List [ float ] Affinity scores of the training interactions. required Source code in pydebiaseddta\\guides\\bowdta.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training ligands. train_labels : List[float] Affinity scores of the training interactions. \"\"\" tokenized_ligands = self . tokenize_ligands ( train_ligands ) tokenized_proteins = self . tokenize_proteins ( train_proteins ) self . ligand_bow_vectorizer . fit_on_texts ( tokenized_ligands ) self . protein_bow_vectorizer . fit_on_texts ( tokenized_proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) X_train = np . hstack ([ ligand_vectors , protein_vectors ]) self . prediction_model . fit ( X_train , train_labels ) vectorize_ligands ( smiles_words ) Computes bag-of-words vectors of the ligands based on their frequency. Parameters: Name Type Description Default smiles_words List [ List [ int ]] ligand words of each ligand as a sequence of sequences. required Returns: Type Description np . array Bag-of-words vectors stacked in a matrix. Source code in pydebiaseddta\\guides\\bowdta.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def vectorize_ligands ( self , smiles_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the ligands based on their frequency. Parameters ---------- smiles_words : List[List[int]] ligand words of each ligand as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . ligand_bow_vectorizer . texts_to_matrix ( smiles_words , mode = \"freq\" ) vectorize_proteins ( protein_words ) Computes bag-of-words vectors of the proteins based on their frequency. Parameters: Name Type Description Default protein_words List [ List [ int ]] Protein words of each protein as a sequence of sequences. required Returns: Type Description np . array Bag-of-words vectors stacked in a matrix. Source code in pydebiaseddta\\guides\\bowdta.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def vectorize_proteins ( self , protein_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the proteins based on their frequency. Parameters ---------- protein_words : List[List[int]] Protein words of each protein as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . protein_bow_vectorizer . texts_to_matrix ( protein_words , mode = \"freq\" ) IDDTA Bases: Guide Source code in pydebiaseddta\\guides\\iddta.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class IDDTA ( Guide ): def __init__ ( self ): \"\"\"Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. \"\"\" self . prediction_model = DecisionTreeRegressor () self . ligand_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) self . protein_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the ligands. Parameters ---------- ligands : List[str] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). Returns ------- np.array One-hot encoded vectors of the ligands. \"\"\" ligands = np . array ( ligands ) . reshape ( - 1 , 1 ) return self . ligand_encoder . transform ( ligands ) . todense () def vectorize_proteins ( self , proteins : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the proteins. Parameters ---------- proteins : List[str] Amino-acid sequences of the input proteins. Returns ------- np.array One-hot encoded vectors of the proteins. \"\"\" proteins = np . array ( proteins ) . reshape ( - 1 , 1 ) return self . protein_encoder . transform ( proteins ) . todense () def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the interactions. \"\"\" ligand_vecs = self . ligand_encoder . fit_transform ( _list_to_numpy ( train_ligands ) ) . todense () protein_vecs = self . protein_encoder . fit_transform ( _list_to_numpy ( train_proteins ) ) . todense () X_train = np . hstack ([ ligand_vecs , protein_vecs ]) self . prediction_model . fit ( X_train , train_labels ) def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" ligand_vecs = self . vectorize_ligands ( ligands ) protein_vecs = self . vectorize_proteins ( proteins ) X_test = np . hstack ([ ligand_vecs , protein_vecs ]) return self . prediction_model . predict ( X_test ) __init__ () Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. Source code in pydebiaseddta\\guides\\iddta.py 14 15 16 17 18 19 20 21 def __init__ ( self ): \"\"\"Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. \"\"\" self . prediction_model = DecisionTreeRegressor () self . ligand_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) self . protein_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) predict ( ligands , proteins ) Predicts the affinities of a list of protein-ligand pairs. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinities. Source code in pydebiaseddta\\guides\\iddta.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" ligand_vecs = self . vectorize_ligands ( ligands ) protein_vecs = self . vectorize_proteins ( proteins ) X_test = np . hstack ([ ligand_vecs , protein_vecs ]) return self . prediction_model . predict ( X_test ) train ( train_ligands , train_proteins , train_labels ) Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the interactions. required Source code in pydebiaseddta\\guides\\iddta.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the interactions. \"\"\" ligand_vecs = self . ligand_encoder . fit_transform ( _list_to_numpy ( train_ligands ) ) . todense () protein_vecs = self . protein_encoder . fit_transform ( _list_to_numpy ( train_proteins ) ) . todense () X_train = np . hstack ([ ligand_vecs , protein_vecs ]) self . prediction_model . fit ( X_train , train_labels ) vectorize_ligands ( ligands ) Creates one-hot vectors of the ligands. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). required Returns: Type Description np . array One-hot encoded vectors of the ligands. Source code in pydebiaseddta\\guides\\iddta.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the ligands. Parameters ---------- ligands : List[str] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). Returns ------- np.array One-hot encoded vectors of the ligands. \"\"\" ligands = np . array ( ligands ) . reshape ( - 1 , 1 ) return self . ligand_encoder . transform ( ligands ) . todense () vectorize_proteins ( proteins ) Creates one-hot vectors of the proteins. Parameters: Name Type Description Default proteins List [ str ] Amino-acid sequences of the input proteins. required Returns: Type Description np . array One-hot encoded vectors of the proteins. Source code in pydebiaseddta\\guides\\iddta.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def vectorize_proteins ( self , proteins : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the proteins. Parameters ---------- proteins : List[str] Amino-acid sequences of the input proteins. Returns ------- np.array One-hot encoded vectors of the proteins. \"\"\" proteins = np . array ( proteins ) . reshape ( - 1 , 1 ) return self . protein_encoder . transform ( proteins ) . todense ()","title":"guides"},{"location":"api/guides/#guides","text":"The submodule that contains the guides, i.e. , the weak learners in DebiasedDTA that learn a weighting of the training set to improve generalizability. The implemented guides are IDDTA and BoWDTA, and an abstract classes is also available to quickly implement custom guides.","title":"guides"},{"location":"api/guides/#pydebiaseddta.guides.abstract_guide.Guide","text":"Bases: ABC An abstract class that implements the interface of a guide in pydebiaseddta . The guides are characterized by a train function and a predict function, whose signatures are implemented by this class. Any instance of the Guide class can be trained in the DebiasedDTA training framework, and therefore, Guide can be inherited to design custom guide models. Source code in pydebiaseddta\\guides\\abstract_guide.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class Guide ( ABC ): \"\"\"An abstract class that implements the interface of a guide in `pydebiaseddta`. The guides are characterized by a `train` function and a `predict` function, whose signatures are implemented by this class. Any instance of the `Guide` class can be trained in the `DebiasedDTA` training framework, and therefore, `Guide` can be inherited to design custom guide models. \"\"\" @abstractmethod def train ( train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], ): \"\"\"An abstract method to define the training interface of the guides. Parameters ---------- train_ligands : List[Any] Training ligands in any representation. train_proteins : List[Any] Training proteins in any representation. train_labels : List[float] Affinity scores of the training protein-ligand pairs. \"\"\" pass @abstractmethod def predict ( ligands : List [ Any ], proteins : List [ Any ]) -> List [ float ]: \"\"\"An abstract method to define the prediction interface of the guides. Parameters ---------- ligands : List[Any] Ligands in any representation. proteins : List[Any] Proteins in any representation. Returns ------- List[float] The predicted affinities. \"\"\" pass","title":"Guide"},{"location":"api/guides/#pydebiaseddta.guides.abstract_guide.Guide.predict","text":"An abstract method to define the prediction interface of the guides. Parameters: Name Type Description Default ligands List [ Any ] Ligands in any representation. required proteins List [ Any ] Proteins in any representation. required Returns: Type Description List [ float ] The predicted affinities. Source code in pydebiaseddta\\guides\\abstract_guide.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @abstractmethod def predict ( ligands : List [ Any ], proteins : List [ Any ]) -> List [ float ]: \"\"\"An abstract method to define the prediction interface of the guides. Parameters ---------- ligands : List[Any] Ligands in any representation. proteins : List[Any] Proteins in any representation. Returns ------- List[float] The predicted affinities. \"\"\" pass","title":"predict()"},{"location":"api/guides/#pydebiaseddta.guides.abstract_guide.Guide.train","text":"An abstract method to define the training interface of the guides. Parameters: Name Type Description Default train_ligands List [ Any ] Training ligands in any representation. required train_proteins List [ Any ] Training proteins in any representation. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required Source code in pydebiaseddta\\guides\\abstract_guide.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @abstractmethod def train ( train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], ): \"\"\"An abstract method to define the training interface of the guides. Parameters ---------- train_ligands : List[Any] Training ligands in any representation. train_proteins : List[Any] Training proteins in any representation. train_labels : List[float] Affinity scores of the training protein-ligand pairs. \"\"\" pass","title":"train()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA","text":"Bases: Guide Source code in pydebiaseddta\\guides\\bowdta.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class BoWDTA ( Guide ): def __init__ ( self ): \"\"\"Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. \"\"\" self . ligand_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"C\" ) self . protein_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"$\" ) self . prediction_model = DecisionTreeRegressor () def tokenize_ligands ( self , smiles : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters ---------- smiles : List[str] The SMILES strings of the ligands Returns ------- List[List[int]] Label encoded sequences of ligand words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( smiles , smi_to_unichar_encoding ) word_identifier = load_ligand_word_identifier ( vocab_size = 8000 ) return word_identifier . encode_sequences ( unichars , 100 ) def tokenize_proteins ( self , aa_sequences : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- List[List[int]] Label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return word_identifier . encode_sequences ( aa_sequences , 1000 ) def vectorize_ligands ( self , smiles_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the ligands based on their frequency. Parameters ---------- smiles_words : List[List[int]] ligand words of each ligand as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . ligand_bow_vectorizer . texts_to_matrix ( smiles_words , mode = \"freq\" ) def vectorize_proteins ( self , protein_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the proteins based on their frequency. Parameters ---------- protein_words : List[List[int]] Protein words of each protein as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . protein_bow_vectorizer . texts_to_matrix ( protein_words , mode = \"freq\" ) def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training ligands. train_labels : List[float] Affinity scores of the training interactions. \"\"\" tokenized_ligands = self . tokenize_ligands ( train_ligands ) tokenized_proteins = self . tokenize_proteins ( train_proteins ) self . ligand_bow_vectorizer . fit_on_texts ( tokenized_ligands ) self . protein_bow_vectorizer . fit_on_texts ( tokenized_proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) X_train = np . hstack ([ ligand_vectors , protein_vectors ]) self . prediction_model . fit ( X_train , train_labels ) def predict ( self , ligands : List [ str ], proteins : List [ str ] ) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" tokenized_ligands = self . tokenize_ligands ( ligands ) tokenized_proteins = self . tokenize_proteins ( proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) interaction = np . hstack ([ ligand_vectors , protein_vectors ]) return self . prediction_model . predict ( interaction ) . tolist ()","title":"BoWDTA"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.__init__","text":"Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. Source code in pydebiaseddta\\guides\\bowdta.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self ): \"\"\"Constructor to create a BoWDTA model. BoWDTA represents the proteins and ligands as \"bag-of-words` and uses a decision tree for prediction. BoWDTA uses the same biomolecule vocabulary as BPEDTA. \"\"\" self . ligand_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"C\" ) self . protein_bow_vectorizer = Tokenizer ( filters = None , lower = False , oov_token = \"$\" ) self . prediction_model = DecisionTreeRegressor ()","title":"__init__()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.predict","text":"Predicts the affinities of a list of protein-ligand pairs. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinities. Source code in pydebiaseddta\\guides\\bowdta.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def predict ( self , ligands : List [ str ], proteins : List [ str ] ) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" tokenized_ligands = self . tokenize_ligands ( ligands ) tokenized_proteins = self . tokenize_proteins ( proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) interaction = np . hstack ([ ligand_vectors , protein_vectors ]) return self . prediction_model . predict ( interaction ) . tolist ()","title":"predict()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.tokenize_ligands","text":"Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters: Name Type Description Default smiles List [ str ] The SMILES strings of the ligands required Returns: Type Description List [ List [ int ]] Label encoded sequences of ligand words. Source code in pydebiaseddta\\guides\\bowdta.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def tokenize_ligands ( self , smiles : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments SMILES strings of the ligands into their ligand words and applies label encoding. Parameters ---------- smiles : List[str] The SMILES strings of the ligands Returns ------- List[List[int]] Label encoded sequences of ligand words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( smiles , smi_to_unichar_encoding ) word_identifier = load_ligand_word_identifier ( vocab_size = 8000 ) return word_identifier . encode_sequences ( unichars , 100 )","title":"tokenize_ligands()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.tokenize_proteins","text":"Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of the proteins. required Returns: Type Description List [ List [ int ]] Label encoded sequences of protein words. Source code in pydebiaseddta\\guides\\bowdta.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def tokenize_proteins ( self , aa_sequences : List [ str ]) -> List [ List [ int ]]: \"\"\"Segments amino-acid sequences of the proteins into their protein words and applies label encoding. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- List[List[int]] Label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return word_identifier . encode_sequences ( aa_sequences , 1000 )","title":"tokenize_proteins()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.train","text":"Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training ligands. required train_labels List [ float ] Affinity scores of the training interactions. required Source code in pydebiaseddta\\guides\\bowdta.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains a BoWDTA model on the provided protein-ligand interactions. The biomolecules are represented as bag of their biomolecule words and a decision tree is used for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training ligands. train_labels : List[float] Affinity scores of the training interactions. \"\"\" tokenized_ligands = self . tokenize_ligands ( train_ligands ) tokenized_proteins = self . tokenize_proteins ( train_proteins ) self . ligand_bow_vectorizer . fit_on_texts ( tokenized_ligands ) self . protein_bow_vectorizer . fit_on_texts ( tokenized_proteins ) ligand_vectors = self . vectorize_ligands ( tokenized_ligands ) protein_vectors = self . vectorize_proteins ( tokenized_proteins ) X_train = np . hstack ([ ligand_vectors , protein_vectors ]) self . prediction_model . fit ( X_train , train_labels )","title":"train()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.vectorize_ligands","text":"Computes bag-of-words vectors of the ligands based on their frequency. Parameters: Name Type Description Default smiles_words List [ List [ int ]] ligand words of each ligand as a sequence of sequences. required Returns: Type Description np . array Bag-of-words vectors stacked in a matrix. Source code in pydebiaseddta\\guides\\bowdta.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def vectorize_ligands ( self , smiles_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the ligands based on their frequency. Parameters ---------- smiles_words : List[List[int]] ligand words of each ligand as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . ligand_bow_vectorizer . texts_to_matrix ( smiles_words , mode = \"freq\" )","title":"vectorize_ligands()"},{"location":"api/guides/#pydebiaseddta.guides.bowdta.BoWDTA.vectorize_proteins","text":"Computes bag-of-words vectors of the proteins based on their frequency. Parameters: Name Type Description Default protein_words List [ List [ int ]] Protein words of each protein as a sequence of sequences. required Returns: Type Description np . array Bag-of-words vectors stacked in a matrix. Source code in pydebiaseddta\\guides\\bowdta.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def vectorize_proteins ( self , protein_words : List [ List [ int ]]) -> np . array : \"\"\"Computes bag-of-words vectors of the proteins based on their frequency. Parameters ---------- protein_words : List[List[int]] Protein words of each protein as a sequence of sequences. Returns ------- np.array Bag-of-words vectors stacked in a matrix. \"\"\" return self . protein_bow_vectorizer . texts_to_matrix ( protein_words , mode = \"freq\" )","title":"vectorize_proteins()"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA","text":"Bases: Guide Source code in pydebiaseddta\\guides\\iddta.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class IDDTA ( Guide ): def __init__ ( self ): \"\"\"Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. \"\"\" self . prediction_model = DecisionTreeRegressor () self . ligand_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) self . protein_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the ligands. Parameters ---------- ligands : List[str] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). Returns ------- np.array One-hot encoded vectors of the ligands. \"\"\" ligands = np . array ( ligands ) . reshape ( - 1 , 1 ) return self . ligand_encoder . transform ( ligands ) . todense () def vectorize_proteins ( self , proteins : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the proteins. Parameters ---------- proteins : List[str] Amino-acid sequences of the input proteins. Returns ------- np.array One-hot encoded vectors of the proteins. \"\"\" proteins = np . array ( proteins ) . reshape ( - 1 , 1 ) return self . protein_encoder . transform ( proteins ) . todense () def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the interactions. \"\"\" ligand_vecs = self . ligand_encoder . fit_transform ( _list_to_numpy ( train_ligands ) ) . todense () protein_vecs = self . protein_encoder . fit_transform ( _list_to_numpy ( train_proteins ) ) . todense () X_train = np . hstack ([ ligand_vecs , protein_vecs ]) self . prediction_model . fit ( X_train , train_labels ) def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" ligand_vecs = self . vectorize_ligands ( ligands ) protein_vecs = self . vectorize_proteins ( proteins ) X_test = np . hstack ([ ligand_vecs , protein_vecs ]) return self . prediction_model . predict ( X_test )","title":"IDDTA"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA.__init__","text":"Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. Source code in pydebiaseddta\\guides\\iddta.py 14 15 16 17 18 19 20 21 def __init__ ( self ): \"\"\"Constructor to create an IDDTA model. IDDTA represents the proteins and ligands with one-hot vectors of their identities and uses a decision tree for prediction. \"\"\" self . prediction_model = DecisionTreeRegressor () self . ligand_encoder = OneHotEncoder ( handle_unknown = \"ignore\" ) self . protein_encoder = OneHotEncoder ( handle_unknown = \"ignore\" )","title":"__init__()"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA.predict","text":"Predicts the affinities of a list of protein-ligand pairs. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinities. Source code in pydebiaseddta\\guides\\iddta.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a list of protein-ligand pairs. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinities. \"\"\" ligand_vecs = self . vectorize_ligands ( ligands ) protein_vecs = self . vectorize_proteins ( proteins ) X_test = np . hstack ([ ligand_vecs , protein_vecs ]) return self . prediction_model . predict ( X_test )","title":"predict()"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA.train","text":"Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the interactions. required Source code in pydebiaseddta\\guides\\iddta.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], ): \"\"\"Trains the IDDTA model. IDDTA represents the biomolecules with one-hot-encoding of their identities and applies decision tree for affinity prediction. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the interactions. \"\"\" ligand_vecs = self . ligand_encoder . fit_transform ( _list_to_numpy ( train_ligands ) ) . todense () protein_vecs = self . protein_encoder . fit_transform ( _list_to_numpy ( train_proteins ) ) . todense () X_train = np . hstack ([ ligand_vecs , protein_vecs ]) self . prediction_model . fit ( X_train , train_labels )","title":"train()"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA.vectorize_ligands","text":"Creates one-hot vectors of the ligands. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). required Returns: Type Description np . array One-hot encoded vectors of the ligands. Source code in pydebiaseddta\\guides\\iddta.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the ligands. Parameters ---------- ligands : List[str] SMILES strings of the input ligands (other representations are also possible, but SMILES is used in this study). Returns ------- np.array One-hot encoded vectors of the ligands. \"\"\" ligands = np . array ( ligands ) . reshape ( - 1 , 1 ) return self . ligand_encoder . transform ( ligands ) . todense ()","title":"vectorize_ligands()"},{"location":"api/guides/#pydebiaseddta.guides.iddta.IDDTA.vectorize_proteins","text":"Creates one-hot vectors of the proteins. Parameters: Name Type Description Default proteins List [ str ] Amino-acid sequences of the input proteins. required Returns: Type Description np . array One-hot encoded vectors of the proteins. Source code in pydebiaseddta\\guides\\iddta.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def vectorize_proteins ( self , proteins : List [ str ]) -> np . array : \"\"\"Creates one-hot vectors of the proteins. Parameters ---------- proteins : List[str] Amino-acid sequences of the input proteins. Returns ------- np.array One-hot encoded vectors of the proteins. \"\"\" proteins = np . array ( proteins ) . reshape ( - 1 , 1 ) return self . protein_encoder . transform ( proteins ) . todense ()","title":"vectorize_proteins()"},{"location":"api/predictors/","text":"predictors The submodule that contains the predictors, i.e. , drug-target affinity (DTA) prediction models, implemented in DebiasedDTA study. The implemented predictors are BPEDTA, DeepDTA, and LMDTA. Abstract classes are also available to quickly train a custom DTA prediction model with DebiasedDTA. Predictor Bases: ABC An abstract class that implements the interface of a predictor in pydebiaseddta . The predictors are characterized by an n_epochs attribute and a train function, whose signatures are implemented by this class. Any instance of Predictor class can be trained in the DebiasedDTA training framework, and therefore, Predictor can be inherited to debias custom DTA prediction models. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Predictor ( ABC ): \"\"\"An abstract class that implements the interface of a predictor in `pydebiaseddta`. The predictors are characterized by an `n_epochs` attribute and a `train` function, whose signatures are implemented by this class. Any instance of `Predictor` class can be trained in the `DebiasedDTA` training framework, and therefore, `Predictor` can be inherited to debias custom DTA prediction models. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass __init__ ( n_epochs , * args , ** kwargs ) abstractmethod An abstract constructor for Predictor to display that n_epochs is a necessary attribute for children classes. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 41 42 43 44 45 46 47 48 49 50 @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs train ( train_ligands , train_proteins , train_labels , val_ligands = None , val_proteins = None , val_labels = None , sample_weights_by_epoch = None ) abstractmethod An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters: Name Type Description Default train_ligands List [ Any ] The training ligands as a List. required train_proteins List [ Any ] The training proteins as a List. required train_labels List [ float ] Affinity scores of the training protein-compound pairs required val_ligands List [ Any ], optional Validation ligands as a List, in case validation scores are measured during training, by default None None val_proteins List [ Any ], optional Validation proteins as a List, in case validation scores are measured during training, by default None None val_labels List [ float ], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default None None Returns: Type Description Any The function is free to return any value after its training, including None . Source code in pydebiaseddta\\predictors\\abstract_predictors.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass TFPredictor Bases: Predictor The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. TFPredictor class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and __init__ functions. Model training, prediction, and save/load functions are inherited from this class. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class TFPredictor ( Predictor ): \"\"\"The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. `TFPredictor` class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and `__init__` functions. Model training, prediction, and save/load functions are inherited from this class. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 ) __init__ ( n_epochs , learning_rate , batch_size , ** kwargs ) abstractmethod An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the build function. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required learning_rate float The learning rate of the optimization algorithm. required batch_size _type_ Batch size for training. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () build () abstractmethod An abstract function to create the model architecture. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 117 118 119 120 121 122 @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass from_file ( path ) classmethod A utility function to load a TFPredictor instance from disk. All attributes, including the model weights, are loaded. Parameters: Name Type Description Default path str Path to load the prediction model from. required Returns: Type Description TFPredictor The previously saved model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance predict ( ligands , proteins ) Predicts the affinities of a List of protein-ligand pairs via the trained DTA prediction model, i.e. , BPE-DTA, LM-DTA, and BPE-DTA. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinity scores by DTA prediction model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () save ( path ) A utility function to save a TFPredictor instance to the disk. All attributes, including the model weights, are saved. Parameters: Name Type Description Default path str Path to save the predictor. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 ) train ( train_ligands , train_proteins , train_labels , val_ligands = None , val_proteins = None , val_labels = None , sample_weights_by_epoch = None ) The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ str ], optional SMILES strings of the validation ligands, by default None and no validation is used. None val_proteins List [ str ], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. None val_labels List [ float ], optional Affinity scores of the validation pairs, by default None and no validation is used. None sample_weights_by_epoch List [ np . array ], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size \\(E\\) (number of training epochs), in which each element is a np.array of \\(N imes 1\\) , where \\(N\\) is the training set size and each element corresponds to the weight of a training sample. By default None and no weighting is used. None Returns: Type Description Dict Training history. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history vectorize_ligands ( ligands ) abstractmethod An abstract function to vectorize ligands. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 124 125 126 127 128 129 @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass vectorize_proteins ( proteins ) abstractmethod An abstract function to vectorize proteins. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 131 132 133 134 135 136 @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass create_uniform_weights ( n_samples , n_epochs ) Create a lists of weights such that every training instance has the equal weight across all epoch, i.e. , no sample weighting is used. Parameters: Name Type Description Default n_samples int Number of training instances. required n_epochs int Number of epochs to train the model. required Returns: Type Description List [ np . array ] Sample weights across epochs. Each instance has a weight of 1 for all epochs. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def create_uniform_weights ( n_samples : int , n_epochs : int ) -> List [ np . array ]: \"\"\"Create a lists of weights such that every training instance has the equal weight across all epoch, *i.e.*, no sample weighting is used. Parameters ---------- n_samples : int Number of training instances. n_epochs : int Number of epochs to train the model. Returns ------- List[np.array] Sample weights across epochs. Each instance has a weight of 1 for all epochs. \"\"\" return [ np . array ([ 1 ] * n_samples ) for _ in range ( n_epochs )] DeepDTA Bases: TFPredictor Source code in pydebiaseddta\\predictors\\deepdta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DeepDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) __init__ ( max_smi_len = 100 , max_prot_len = 1000 , embedding_dim = 128 , learning_rate = 0.001 , batch_size = 256 , n_epochs = 200 , num_filters = 32 , smi_filter_len = 4 , prot_filter_len = 6 ) Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. 100 max_prot_len int , optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule characters, by default 128. 128 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\deepdta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) build () Builds a DeepDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\deepdta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta vectorize_ligands ( ligands ) Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. Source code in pydebiaseddta\\predictors\\deepdta.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) vectorize_proteins ( aa_sequences ) Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. Source code in pydebiaseddta\\predictors\\deepdta.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) BPEDTA Bases: TFPredictor Source code in pydebiaseddta\\predictors\\bpedta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class BPEDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) __init__ ( max_smi_len = 100 , max_prot_len = 1000 , embedding_dim = 128 , learning_rate = 0.001 , batch_size = 256 , n_epochs = 200 , num_filters = 32 , smi_filter_len = 4 , prot_filter_len = 6 ) Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. 100 max_prot_len int , optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule words, by default 128. 128 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\bpedta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) build () Builds a BPEDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\bpedta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta vectorize_ligands ( ligands ) Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of chemical words. Source code in pydebiaseddta\\predictors\\bpedta.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) vectorize_proteins ( aa_sequences ) Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of protein words. Source code in pydebiaseddta\\predictors\\bpedta.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) LMDTA Bases: TFPredictor Source code in pydebiaseddta\\predictors\\lmdta.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LMDTA ( TFPredictor ): def __init__ ( self , n_epochs : int = 200 , learning_rate : float = 0.001 , batch_size : int = 256 ): \"\"\"Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via [`ChemBERTa`](https://arxiv.org/abs/2010.09885) and [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters ---------- n_epochs : int, optional Number of epochs to train the model, by default 200. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. \"\"\" transformers . logging . set_verbosity ( transformers . logging . CRITICAL ) self . chemical_tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . chemberta = AutoModel . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . protein_tokenizer = AutoTokenizer . from_pretrained ( \"Rostlab/prot_bert\" , do_lower_case = False ) self . protbert = AutoModel . from_pretrained ( \"Rostlab/prot_bert\" ) TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `LMDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" chemicals = Input ( shape = ( 768 ,), dtype = \"float32\" ) proteins = Input ( shape = ( 1024 ,), dtype = \"float32\" ) interaction_representation = Concatenate ( axis =- 1 )([ chemicals , proteins ]) FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 512 , activation = \"relu\" )( FC1 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC2 ) opt = Adam ( self . learning_rate ) lmdta = Model ( inputs = [ chemicals , proteins ], outputs = [ predictions ]) lmdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ] ) return lmdta @lru_cache ( maxsize = 2048 ) def get_chemberta_embedding ( self , smiles : str ) -> np . array : \"\"\"Computes the [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector for a ligand. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- smiles : str SMILES string of the ligand. Returns ------- np.array [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector (768-dimensional) of the ligand. \"\"\" tokens = self . chemical_tokenizer ( smiles , return_tensors = \"pt\" ) output = self . chemberta ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Vectorizes the ligands with [`ChemBERTa`](https://arxiv.org/abs/2010.09885) embeddings. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times 768$ ($N$ is the number of the input ligands) matrix that contains [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_chemberta_embedding ( chemical ) for chemical in ligands ] ) @lru_cache ( maxsize = 1024 ) def get_protbert_embedding ( self , aa_sequence : str ) -> np . array : \"\"\"Computes the [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector for a protein. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- aa_sequence : str Amino-acid sequence of the protein. Returns ------- np.array [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector (1024-dimensional) of the protein. \"\"\" pp_sequence = \" \" . join ( aa_sequence ) cleaned_sequence = re . sub ( r \"[UZOB]\" , \"X\" , pp_sequence ) tokens = self . protein_tokenizer ( cleaned_sequence , return_tensors = \"pt\" ) output = self . protbert ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Vectorizes the proteins with [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) embeddings. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- np.array An $N \\\\times 1024$ ($N$ is the number of the input proteins) matrix that contains [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_protbert_embedding ( aa_sequence ) for aa_sequence in aa_sequences ] ) __init__ ( n_epochs = 200 , learning_rate = 0.001 , batch_size = 256 ) Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via ChemBERTa and ProtBert models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters: Name Type Description Default n_epochs int , optional Number of epochs to train the model, by default 200. 200 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 Source code in pydebiaseddta\\predictors\\lmdta.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , n_epochs : int = 200 , learning_rate : float = 0.001 , batch_size : int = 256 ): \"\"\"Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via [`ChemBERTa`](https://arxiv.org/abs/2010.09885) and [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters ---------- n_epochs : int, optional Number of epochs to train the model, by default 200. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. \"\"\" transformers . logging . set_verbosity ( transformers . logging . CRITICAL ) self . chemical_tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . chemberta = AutoModel . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . protein_tokenizer = AutoTokenizer . from_pretrained ( \"Rostlab/prot_bert\" , do_lower_case = False ) self . protbert = AutoModel . from_pretrained ( \"Rostlab/prot_bert\" ) TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) build () Builds a LMDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\lmdta.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def build ( self ): \"\"\"Builds a `LMDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" chemicals = Input ( shape = ( 768 ,), dtype = \"float32\" ) proteins = Input ( shape = ( 1024 ,), dtype = \"float32\" ) interaction_representation = Concatenate ( axis =- 1 )([ chemicals , proteins ]) FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 512 , activation = \"relu\" )( FC1 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC2 ) opt = Adam ( self . learning_rate ) lmdta = Model ( inputs = [ chemicals , proteins ], outputs = [ predictions ]) lmdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ] ) return lmdta get_chemberta_embedding ( smiles ) cached Computes the ChemBERTa vector for a ligand. Since the creating the vector is computation-heavy, an lru_cache of size 2048 is used to store produced vectors. Parameters: Name Type Description Default smiles str SMILES string of the ligand. required Returns: Type Description np . array ChemBERTa vector (768-dimensional) of the ligand. Source code in pydebiaseddta\\predictors\\lmdta.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @lru_cache ( maxsize = 2048 ) def get_chemberta_embedding ( self , smiles : str ) -> np . array : \"\"\"Computes the [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector for a ligand. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- smiles : str SMILES string of the ligand. Returns ------- np.array [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector (768-dimensional) of the ligand. \"\"\" tokens = self . chemical_tokenizer ( smiles , return_tensors = \"pt\" ) output = self . chemberta ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) get_protbert_embedding ( aa_sequence ) cached Computes the ProtBert vector for a protein. Since the creating the vector is computation-heavy, an lru_cache of size 2048 is used to store produced vectors. Parameters: Name Type Description Default aa_sequence str Amino-acid sequence of the protein. required Returns: Type Description np . array ProtBert vector (1024-dimensional) of the protein. Source code in pydebiaseddta\\predictors\\lmdta.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @lru_cache ( maxsize = 1024 ) def get_protbert_embedding ( self , aa_sequence : str ) -> np . array : \"\"\"Computes the [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector for a protein. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- aa_sequence : str Amino-acid sequence of the protein. Returns ------- np.array [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector (1024-dimensional) of the protein. \"\"\" pp_sequence = \" \" . join ( aa_sequence ) cleaned_sequence = re . sub ( r \"[UZOB]\" , \"X\" , pp_sequence ) tokens = self . protein_tokenizer ( cleaned_sequence , return_tensors = \"pt\" ) output = self . protbert ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) vectorize_ligands ( ligands ) Vectorizes the ligands with ChemBERTa embeddings. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times 768\\) ( \\(N\\) is the number of the input ligands) matrix that contains ChemBERTa vectors of the ligands. Source code in pydebiaseddta\\predictors\\lmdta.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Vectorizes the ligands with [`ChemBERTa`](https://arxiv.org/abs/2010.09885) embeddings. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times 768$ ($N$ is the number of the input ligands) matrix that contains [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_chemberta_embedding ( chemical ) for chemical in ligands ] ) vectorize_proteins ( aa_sequences ) Vectorizes the proteins with ProtBert embeddings. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of the proteins. required Returns: Type Description np . array An \\(N \\times 1024\\) ( \\(N\\) is the number of the input proteins) matrix that contains ProtBert vectors of the ligands. Source code in pydebiaseddta\\predictors\\lmdta.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Vectorizes the proteins with [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) embeddings. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- np.array An $N \\\\times 1024$ ($N$ is the number of the input proteins) matrix that contains [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_protbert_embedding ( aa_sequence ) for aa_sequence in aa_sequences ] )","title":"predictors"},{"location":"api/predictors/#predictors","text":"The submodule that contains the predictors, i.e. , drug-target affinity (DTA) prediction models, implemented in DebiasedDTA study. The implemented predictors are BPEDTA, DeepDTA, and LMDTA. Abstract classes are also available to quickly train a custom DTA prediction model with DebiasedDTA.","title":"predictors"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor","text":"Bases: ABC An abstract class that implements the interface of a predictor in pydebiaseddta . The predictors are characterized by an n_epochs attribute and a train function, whose signatures are implemented by this class. Any instance of Predictor class can be trained in the DebiasedDTA training framework, and therefore, Predictor can be inherited to debias custom DTA prediction models. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Predictor ( ABC ): \"\"\"An abstract class that implements the interface of a predictor in `pydebiaseddta`. The predictors are characterized by an `n_epochs` attribute and a `train` function, whose signatures are implemented by this class. Any instance of `Predictor` class can be trained in the `DebiasedDTA` training framework, and therefore, `Predictor` can be inherited to debias custom DTA prediction models. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass","title":"Predictor"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor.__init__","text":"An abstract constructor for Predictor to display that n_epochs is a necessary attribute for children classes. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 41 42 43 44 45 46 47 48 49 50 @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor.train","text":"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters: Name Type Description Default train_ligands List [ Any ] The training ligands as a List. required train_proteins List [ Any ] The training proteins as a List. required train_labels List [ float ] Affinity scores of the training protein-compound pairs required val_ligands List [ Any ], optional Validation ligands as a List, in case validation scores are measured during training, by default None None val_proteins List [ Any ], optional Validation proteins as a List, in case validation scores are measured during training, by default None None val_labels List [ float ], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default None None Returns: Type Description Any The function is free to return any value after its training, including None . Source code in pydebiaseddta\\predictors\\abstract_predictors.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass","title":"train()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor","text":"Bases: Predictor The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. TFPredictor class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and __init__ functions. Model training, prediction, and save/load functions are inherited from this class. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class TFPredictor ( Predictor ): \"\"\"The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. `TFPredictor` class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and `__init__` functions. Model training, prediction, and save/load functions are inherited from this class. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 )","title":"TFPredictor"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.__init__","text":"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the build function. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required learning_rate float The learning rate of the optimization algorithm. required batch_size _type_ Batch size for training. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build ()","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.build","text":"An abstract function to create the model architecture. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 117 118 119 120 121 122 @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.from_file","text":"A utility function to load a TFPredictor instance from disk. All attributes, including the model weights, are loaded. Parameters: Name Type Description Default path str Path to load the prediction model from. required Returns: Type Description TFPredictor The previously saved model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance","title":"from_file()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.predict","text":"Predicts the affinities of a List of protein-ligand pairs via the trained DTA prediction model, i.e. , BPE-DTA, LM-DTA, and BPE-DTA. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinity scores by DTA prediction model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist ()","title":"predict()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.save","text":"A utility function to save a TFPredictor instance to the disk. All attributes, including the model weights, are saved. Parameters: Name Type Description Default path str Path to save the predictor. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 )","title":"save()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.train","text":"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ str ], optional SMILES strings of the validation ligands, by default None and no validation is used. None val_proteins List [ str ], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. None val_labels List [ float ], optional Affinity scores of the validation pairs, by default None and no validation is used. None sample_weights_by_epoch List [ np . array ], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size \\(E\\) (number of training epochs), in which each element is a np.array of \\(N imes 1\\) , where \\(N\\) is the training set size and each element corresponds to the weight of a training sample. By default None and no weighting is used. None Returns: Type Description Dict Training history. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history","title":"train()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.vectorize_ligands","text":"An abstract function to vectorize ligands. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 124 125 126 127 128 129 @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.vectorize_proteins","text":"An abstract function to vectorize proteins. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 131 132 133 134 135 136 @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass","title":"vectorize_proteins()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.create_uniform_weights","text":"Create a lists of weights such that every training instance has the equal weight across all epoch, i.e. , no sample weighting is used. Parameters: Name Type Description Default n_samples int Number of training instances. required n_epochs int Number of epochs to train the model. required Returns: Type Description List [ np . array ] Sample weights across epochs. Each instance has a weight of 1 for all epochs. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def create_uniform_weights ( n_samples : int , n_epochs : int ) -> List [ np . array ]: \"\"\"Create a lists of weights such that every training instance has the equal weight across all epoch, *i.e.*, no sample weighting is used. Parameters ---------- n_samples : int Number of training instances. n_epochs : int Number of epochs to train the model. Returns ------- List[np.array] Sample weights across epochs. Each instance has a weight of 1 for all epochs. \"\"\" return [ np . array ([ 1 ] * n_samples ) for _ in range ( n_epochs )]","title":"create_uniform_weights()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA","text":"Bases: TFPredictor Source code in pydebiaseddta\\predictors\\deepdta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DeepDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"DeepDTA"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.__init__","text":"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. 100 max_prot_len int , optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule characters, by default 128. 128 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\deepdta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size )","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.build","text":"Builds a DeepDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\deepdta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.vectorize_ligands","text":"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. Source code in pydebiaseddta\\predictors\\deepdta.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) )","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.vectorize_proteins","text":"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. Source code in pydebiaseddta\\predictors\\deepdta.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"vectorize_proteins()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA","text":"Bases: TFPredictor Source code in pydebiaseddta\\predictors\\bpedta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class BPEDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"BPEDTA"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.__init__","text":"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. 100 max_prot_len int , optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule words, by default 128. 128 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\bpedta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size )","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.build","text":"Builds a BPEDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\bpedta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.vectorize_ligands","text":"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of chemical words. Source code in pydebiaseddta\\predictors\\bpedta.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ))","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.vectorize_proteins","text":"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of protein words. Source code in pydebiaseddta\\predictors\\bpedta.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"vectorize_proteins()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA","text":"Bases: TFPredictor Source code in pydebiaseddta\\predictors\\lmdta.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class LMDTA ( TFPredictor ): def __init__ ( self , n_epochs : int = 200 , learning_rate : float = 0.001 , batch_size : int = 256 ): \"\"\"Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via [`ChemBERTa`](https://arxiv.org/abs/2010.09885) and [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters ---------- n_epochs : int, optional Number of epochs to train the model, by default 200. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. \"\"\" transformers . logging . set_verbosity ( transformers . logging . CRITICAL ) self . chemical_tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . chemberta = AutoModel . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . protein_tokenizer = AutoTokenizer . from_pretrained ( \"Rostlab/prot_bert\" , do_lower_case = False ) self . protbert = AutoModel . from_pretrained ( \"Rostlab/prot_bert\" ) TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `LMDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" chemicals = Input ( shape = ( 768 ,), dtype = \"float32\" ) proteins = Input ( shape = ( 1024 ,), dtype = \"float32\" ) interaction_representation = Concatenate ( axis =- 1 )([ chemicals , proteins ]) FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 512 , activation = \"relu\" )( FC1 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC2 ) opt = Adam ( self . learning_rate ) lmdta = Model ( inputs = [ chemicals , proteins ], outputs = [ predictions ]) lmdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ] ) return lmdta @lru_cache ( maxsize = 2048 ) def get_chemberta_embedding ( self , smiles : str ) -> np . array : \"\"\"Computes the [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector for a ligand. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- smiles : str SMILES string of the ligand. Returns ------- np.array [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector (768-dimensional) of the ligand. \"\"\" tokens = self . chemical_tokenizer ( smiles , return_tensors = \"pt\" ) output = self . chemberta ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Vectorizes the ligands with [`ChemBERTa`](https://arxiv.org/abs/2010.09885) embeddings. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times 768$ ($N$ is the number of the input ligands) matrix that contains [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_chemberta_embedding ( chemical ) for chemical in ligands ] ) @lru_cache ( maxsize = 1024 ) def get_protbert_embedding ( self , aa_sequence : str ) -> np . array : \"\"\"Computes the [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector for a protein. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- aa_sequence : str Amino-acid sequence of the protein. Returns ------- np.array [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector (1024-dimensional) of the protein. \"\"\" pp_sequence = \" \" . join ( aa_sequence ) cleaned_sequence = re . sub ( r \"[UZOB]\" , \"X\" , pp_sequence ) tokens = self . protein_tokenizer ( cleaned_sequence , return_tensors = \"pt\" ) output = self . protbert ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Vectorizes the proteins with [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) embeddings. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- np.array An $N \\\\times 1024$ ($N$ is the number of the input proteins) matrix that contains [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_protbert_embedding ( aa_sequence ) for aa_sequence in aa_sequences ] )","title":"LMDTA"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.__init__","text":"Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via ChemBERTa and ProtBert models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters: Name Type Description Default n_epochs int , optional Number of epochs to train the model, by default 200. 200 learning_rate float , optional Learning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 Source code in pydebiaseddta\\predictors\\lmdta.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , n_epochs : int = 200 , learning_rate : float = 0.001 , batch_size : int = 256 ): \"\"\"Constructor to create a LMDTA instance. LMDTA represents ligands and proteins with pre-trained language model embeddings obtained via [`ChemBERTa`](https://arxiv.org/abs/2010.09885) and [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) models, respectively. A fully-connected neural network with two layers is used afterwards to predict affinities. Parameters ---------- n_epochs : int, optional Number of epochs to train the model, by default 200. learning_rate : float, optional Learning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. \"\"\" transformers . logging . set_verbosity ( transformers . logging . CRITICAL ) self . chemical_tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . chemberta = AutoModel . from_pretrained ( \"seyonec/PubChem10M_SMILES_BPE_450k\" ) self . protein_tokenizer = AutoTokenizer . from_pretrained ( \"Rostlab/prot_bert\" , do_lower_case = False ) self . protbert = AutoModel . from_pretrained ( \"Rostlab/prot_bert\" ) TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size )","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.build","text":"Builds a LMDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\lmdta.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def build ( self ): \"\"\"Builds a `LMDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" chemicals = Input ( shape = ( 768 ,), dtype = \"float32\" ) proteins = Input ( shape = ( 1024 ,), dtype = \"float32\" ) interaction_representation = Concatenate ( axis =- 1 )([ chemicals , proteins ]) FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 512 , activation = \"relu\" )( FC1 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC2 ) opt = Adam ( self . learning_rate ) lmdta = Model ( inputs = [ chemicals , proteins ], outputs = [ predictions ]) lmdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ] ) return lmdta","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.get_chemberta_embedding","text":"Computes the ChemBERTa vector for a ligand. Since the creating the vector is computation-heavy, an lru_cache of size 2048 is used to store produced vectors. Parameters: Name Type Description Default smiles str SMILES string of the ligand. required Returns: Type Description np . array ChemBERTa vector (768-dimensional) of the ligand. Source code in pydebiaseddta\\predictors\\lmdta.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @lru_cache ( maxsize = 2048 ) def get_chemberta_embedding ( self , smiles : str ) -> np . array : \"\"\"Computes the [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector for a ligand. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- smiles : str SMILES string of the ligand. Returns ------- np.array [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vector (768-dimensional) of the ligand. \"\"\" tokens = self . chemical_tokenizer ( smiles , return_tensors = \"pt\" ) output = self . chemberta ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 )","title":"get_chemberta_embedding()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.get_protbert_embedding","text":"Computes the ProtBert vector for a protein. Since the creating the vector is computation-heavy, an lru_cache of size 2048 is used to store produced vectors. Parameters: Name Type Description Default aa_sequence str Amino-acid sequence of the protein. required Returns: Type Description np . array ProtBert vector (1024-dimensional) of the protein. Source code in pydebiaseddta\\predictors\\lmdta.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 @lru_cache ( maxsize = 1024 ) def get_protbert_embedding ( self , aa_sequence : str ) -> np . array : \"\"\"Computes the [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector for a protein. Since the creating the vector is computation-heavy, an `lru_cache` of size 2048 is used to store produced vectors. Parameters ---------- aa_sequence : str Amino-acid sequence of the protein. Returns ------- np.array [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vector (1024-dimensional) of the protein. \"\"\" pp_sequence = \" \" . join ( aa_sequence ) cleaned_sequence = re . sub ( r \"[UZOB]\" , \"X\" , pp_sequence ) tokens = self . protein_tokenizer ( cleaned_sequence , return_tensors = \"pt\" ) output = self . protbert ( ** tokens ) return output . last_hidden_state . detach () . numpy () . mean ( axis = 1 )","title":"get_protbert_embedding()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.vectorize_ligands","text":"Vectorizes the ligands with ChemBERTa embeddings. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times 768\\) ( \\(N\\) is the number of the input ligands) matrix that contains ChemBERTa vectors of the ligands. Source code in pydebiaseddta\\predictors\\lmdta.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Vectorizes the ligands with [`ChemBERTa`](https://arxiv.org/abs/2010.09885) embeddings. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times 768$ ($N$ is the number of the input ligands) matrix that contains [`ChemBERTa`](https://arxiv.org/abs/2010.09885) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_chemberta_embedding ( chemical ) for chemical in ligands ] )","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.lmdta.LMDTA.vectorize_proteins","text":"Vectorizes the proteins with ProtBert embeddings. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of the proteins. required Returns: Type Description np . array An \\(N \\times 1024\\) ( \\(N\\) is the number of the input proteins) matrix that contains ProtBert vectors of the ligands. Source code in pydebiaseddta\\predictors\\lmdta.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Vectorizes the proteins with [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) embeddings. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of the proteins. Returns ------- np.array An $N \\\\times 1024$ ($N$ is the number of the input proteins) matrix that contains [`ProtBert`](https://www.biorxiv.org/content/biorxiv/early/2020/07/21/2020.07.12.199554.full.pdf) vectors of the ligands. \"\"\" return np . vstack ( [ self . get_protbert_embedding ( aa_sequence ) for aa_sequence in aa_sequences ] )","title":"vectorize_proteins()"},{"location":"api/sequence/","text":"sequence The submodule for processing SMILES strings. smiles_processing.py consists of utility function to segment SMILES strings, whereas word_identification.py consists of a class to learn biomolecule words and segment biomolecule sequences into biomolecule words. segment_smiles ( smiles , segment_sq_brackets = True ) Segments a SMILES string into its tokens. Parameters: Name Type Description Default smiles str Input SMILES string. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets ( e.g. [C@@H], [Rb]), too. Set to True to have square brackets and the tokens inside as standalone tokens, e.g. [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to False , whole expression is returned as a single token, e.g. \"[C@@H]\" . Defaults to True . True Returns: Type Description List [ str ] Each element of the SMILES string as a list. Source code in pydebiaseddta\\sequence\\smiles_processing.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def segment_smiles ( smiles : str , segment_sq_brackets : bool = True ) -> List [ str ]: \"\"\"Segments a SMILES string into its tokens. Parameters ---------- smiles : str Input SMILES string. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets (*e.g.* [C@@H], [Rb]), too. Set to `True` to have square brackets and the tokens inside as standalone tokens, *e.g.* [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to `False`, whole expression is returned as a single token, *e.g.* \"[C@@H]\" . Defaults to `True`. Returns ------- List[str] Each element of the SMILES string as a list. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles ) segment_smiles_batch ( smiles_batch , segment_sq_brackets = True ) Segments multiple SMILES strings with a single call by wrapping sequence.smiles_processing.segment_smiles . Parameters: Name Type Description Default smiles_batch List [ str ] List of input SMILES strings. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets. See sequence.smiles_processing.segment_smiles for a more detailed explanation. Defaults to True . True Returns: Type Description List [ List [ str ]] A 2D list of strings where element \\([i][j]\\) corresponds to the \\(j^{th}\\) token of the \\(i^{th}\\) input. Source code in pydebiaseddta\\sequence\\smiles_processing.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segments multiple SMILES strings with a single call by wrapping `sequence.smiles_processing.segment_smiles`. Parameters ---------- smiles_batch : List[str] List of input SMILES strings. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets. See `sequence.smiles_processing.segment_smiles` for a more detailed explanation. Defaults to `True`. Returns ------- List[List[str]] A 2D list of strings where element $[i][j]$ corresponds to the $j^{th}$ token of the $i^{th}$ input. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ] WordIdentifier A versatile class to identify biomolecule words in biomolecule strings. WordIdentifier leverages the Byte Pair Encoding algorithm implemented in the tokenizers library to learn biomolecule vocabularies and segment biomolecule strings into their words. Source code in pydebiaseddta\\sequence\\word_identification.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class WordIdentifier : \"\"\"A versatile class to identify biomolecule words in biomolecule strings. `WordIdentifier` leverages the Byte Pair Encoding algorithm implemented in the `tokenizers` library to learn biomolecule vocabularies and segment biomolecule strings into their words. \"\"\" def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath ) __init__ ( vocab_size ) Creates a WordIdentifier instance. Parameters: Name Type Description Default vocab_size int Size of the biomolecule vocabulary. required Source code in pydebiaseddta\\sequence\\word_identification.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () encode_sequences ( sequences , padding_len = None ) Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required padding_len int , optional The desired length of sequences, by default None . No padding is applied when set to None . None Returns: Type Description List [ List [ int ]] List of the id of the biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] from_file ( loadpath ) classmethod Loads a WordIdentifier from a file. Parameters: Name Type Description Default loadpath str Path to the WordIdentifier file. required Returns: Type Description WordIdentifier Previously saved WordIdentifier Source code in pydebiaseddta\\sequence\\word_identification.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance save ( savepath ) Saves a WordIdentifier instance to disk. Parameters: Name Type Description Default savepath str The path to dump the instance. File extension is added automatically. required Source code in pydebiaseddta\\sequence\\word_identification.py 114 115 116 117 118 119 120 121 122 123 124 def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath ) tokenize_sequences ( sequences ) Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required Returns: Type Description List [ List [ str ]] List of biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] train ( corpus_path ) Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters: Name Type Description Default corpus_path str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. required Source code in pydebiaseddta\\sequence\\word_identification.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) load_chemical_word_identifier ( vocab_size ) A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 94 and 8000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def load_chemical_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 94 and 8000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 94 , 8000 ]: raise ValueError ( \"Supported vocab sizes are 94 and 8000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/chemical\" vocab_path = f \" { protein_vocab_path } /chembl27_enc_94.json\" if vocab_size == 8000 : vocab_path = f \" { protein_vocab_path } /chembl27_enc_bpe_8000.json\" return WordIdentifier . from_file ( vocab_path ) load_protein_word_identifier ( vocab_size ) A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 26 and 32000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def load_protein_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 26 and 32000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 26 , 32000 ]: raise ValueError ( \"Supported vocab sizes are 26 and 32000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/protein\" vocab_path = f \" { protein_vocab_path } /uniprot_26.json\" if vocab_size == 32000 : vocab_path = f \" { protein_vocab_path } /uniprot_bpe_32000.json\" return WordIdentifier . from_file ( vocab_path )","title":"sequence"},{"location":"api/sequence/#sequence","text":"The submodule for processing SMILES strings. smiles_processing.py consists of utility function to segment SMILES strings, whereas word_identification.py consists of a class to learn biomolecule words and segment biomolecule sequences into biomolecule words.","title":"sequence"},{"location":"api/sequence/#pydebiaseddta.sequence.smiles_processing.segment_smiles","text":"Segments a SMILES string into its tokens. Parameters: Name Type Description Default smiles str Input SMILES string. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets ( e.g. [C@@H], [Rb]), too. Set to True to have square brackets and the tokens inside as standalone tokens, e.g. [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to False , whole expression is returned as a single token, e.g. \"[C@@H]\" . Defaults to True . True Returns: Type Description List [ str ] Each element of the SMILES string as a list. Source code in pydebiaseddta\\sequence\\smiles_processing.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def segment_smiles ( smiles : str , segment_sq_brackets : bool = True ) -> List [ str ]: \"\"\"Segments a SMILES string into its tokens. Parameters ---------- smiles : str Input SMILES string. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets (*e.g.* [C@@H], [Rb]), too. Set to `True` to have square brackets and the tokens inside as standalone tokens, *e.g.* [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to `False`, whole expression is returned as a single token, *e.g.* \"[C@@H]\" . Defaults to `True`. Returns ------- List[str] Each element of the SMILES string as a list. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles )","title":"segment_smiles()"},{"location":"api/sequence/#pydebiaseddta.sequence.smiles_processing.segment_smiles_batch","text":"Segments multiple SMILES strings with a single call by wrapping sequence.smiles_processing.segment_smiles . Parameters: Name Type Description Default smiles_batch List [ str ] List of input SMILES strings. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets. See sequence.smiles_processing.segment_smiles for a more detailed explanation. Defaults to True . True Returns: Type Description List [ List [ str ]] A 2D list of strings where element \\([i][j]\\) corresponds to the \\(j^{th}\\) token of the \\(i^{th}\\) input. Source code in pydebiaseddta\\sequence\\smiles_processing.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segments multiple SMILES strings with a single call by wrapping `sequence.smiles_processing.segment_smiles`. Parameters ---------- smiles_batch : List[str] List of input SMILES strings. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets. See `sequence.smiles_processing.segment_smiles` for a more detailed explanation. Defaults to `True`. Returns ------- List[List[str]] A 2D list of strings where element $[i][j]$ corresponds to the $j^{th}$ token of the $i^{th}$ input. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ]","title":"segment_smiles_batch()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier","text":"A versatile class to identify biomolecule words in biomolecule strings. WordIdentifier leverages the Byte Pair Encoding algorithm implemented in the tokenizers library to learn biomolecule vocabularies and segment biomolecule strings into their words. Source code in pydebiaseddta\\sequence\\word_identification.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class WordIdentifier : \"\"\"A versatile class to identify biomolecule words in biomolecule strings. `WordIdentifier` leverages the Byte Pair Encoding algorithm implemented in the `tokenizers` library to learn biomolecule vocabularies and segment biomolecule strings into their words. \"\"\" def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath )","title":"WordIdentifier"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.__init__","text":"Creates a WordIdentifier instance. Parameters: Name Type Description Default vocab_size int Size of the biomolecule vocabulary. required Source code in pydebiaseddta\\sequence\\word_identification.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace ()","title":"__init__()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.encode_sequences","text":"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required padding_len int , optional The desired length of sequences, by default None . No padding is applied when set to None . None Returns: Type Description List [ List [ int ]] List of the id of the biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ]","title":"encode_sequences()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.from_file","text":"Loads a WordIdentifier from a file. Parameters: Name Type Description Default loadpath str Path to the WordIdentifier file. required Returns: Type Description WordIdentifier Previously saved WordIdentifier Source code in pydebiaseddta\\sequence\\word_identification.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance","title":"from_file()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.save","text":"Saves a WordIdentifier instance to disk. Parameters: Name Type Description Default savepath str The path to dump the instance. File extension is added automatically. required Source code in pydebiaseddta\\sequence\\word_identification.py 114 115 116 117 118 119 120 121 122 123 124 def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath )","title":"save()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.tokenize_sequences","text":"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required Returns: Type Description List [ List [ str ]] List of biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ]","title":"tokenize_sequences()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.train","text":"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters: Name Type Description Default corpus_path str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. required Source code in pydebiaseddta\\sequence\\word_identification.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" )","title":"train()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.load_chemical_word_identifier","text":"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 94 and 8000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def load_chemical_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 94 and 8000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 94 , 8000 ]: raise ValueError ( \"Supported vocab sizes are 94 and 8000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/chemical\" vocab_path = f \" { protein_vocab_path } /chembl27_enc_94.json\" if vocab_size == 8000 : vocab_path = f \" { protein_vocab_path } /chembl27_enc_bpe_8000.json\" return WordIdentifier . from_file ( vocab_path )","title":"load_chemical_word_identifier()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.load_protein_word_identifier","text":"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 26 and 32000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def load_protein_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 26 and 32000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 26 , 32000 ]: raise ValueError ( \"Supported vocab sizes are 26 and 32000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/protein\" vocab_path = f \" { protein_vocab_path } /uniprot_26.json\" if vocab_size == 32000 : vocab_path = f \" { protein_vocab_path } /uniprot_bpe_32000.json\" return WordIdentifier . from_file ( vocab_path )","title":"load_protein_word_identifier()"},{"location":"api/utils/","text":"utils load_json ( path ) Loads a json file into a dictionary. Parameters: Name Type Description Default path str Path to the .json file to load. required Returns: Type Description Dict Content of the .json file as a dictionary. Source code in pydebiaseddta\\utils.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_json ( path : str ) -> Dict : \"\"\"Loads a json file into a dictionary. Parameters ---------- path : str Path to the .json file to load. Returns ------- Dict Content of the .json file as a dictionary. \"\"\" with open ( path , \"r\" ) as f : return json . load ( f ) load_sample_dta_data ( mini = False ) Loads a portion of the BDB dataset for fast experimenting. Parameters: Name Type Description Default mini bool , optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to True for fast prototyping and False to quickly train a model. Defaults to False . False Returns: Type Description Dict [ str , List ] The dictionary has three keys: \"train\" , \"val\" , and \"test\" , each corresponding to different folds of the dataset. Each key maps to a list with three elements: list of chemicals , list of proteins , and list of affinity scores . The elements in the same index of the lists correspond to a drug-target affinity measurement. Source code in pydebiaseddta\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_sample_dta_data ( mini : bool = False ) -> Dict [ str , List ]: \"\"\"Loads a portion of [the BDB dataset](https://arxiv.org/pdf/2107.05556.pdf) for fast experimenting. Parameters ---------- mini : bool, optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to `True` for fast prototyping and `False` to quickly train a model. Defaults to `False`. Returns ------- Dict[str, List] The dictionary has three keys: `\"train\"`, `\"val\"`, and `\"test\"`, each corresponding to different folds of the dataset. Each key maps to a list with three elements: *list of chemicals*, *list of proteins*, and *list of affinity scores*. The elements in the same index of the lists correspond to a drug-target affinity measurement. \"\"\" sample_data_path = f \" { package_path } /data/dta_sample_data/dta_sample_data.json\" if mini : sample_data_path = f \" { package_path } /data/dta/dta_sample_data.mini.json\" with open ( sample_data_path ) as f : return json . load ( f ) load_sample_smiles () Returns examples SMILES strings from ChEMBL for testing. Returns: Type Description List [ str ] SMILES examples from ChEMBL. Source code in pydebiaseddta\\utils.py 30 31 32 33 34 35 36 37 38 39 40 def load_sample_smiles () -> List [ str ]: \"\"\"Returns examples SMILES strings from ChEMBL for testing. Returns ------- List[str] SMILES examples from ChEMBL. \"\"\" sample_data_path = f \" { package_path } /data/sequence/chembl27.mini.smiles\" with open ( sample_data_path ) as f : return [ line . strip () for line in f . readlines ()] save_json ( obj , path ) Saves a dictionary in json format. The indent is set to 4 for readability. Parameters: Name Type Description Default obj Dict Dictionary to store. required path str Path to store the .json file. required Returns: Type Description None Source code in pydebiaseddta\\utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json ( obj : Dict , path : str ) -> None : \"\"\"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters ---------- obj : Dict Dictionary to store. path : str Path to store the .json file. Returns ------- None \"\"\" with open ( path , \"w\" ) as f : json . dump ( obj , f , indent = 4 )","title":"utils"},{"location":"api/utils/#utils","text":"","title":"utils"},{"location":"api/utils/#pydebiaseddta.utils.load_json","text":"Loads a json file into a dictionary. Parameters: Name Type Description Default path str Path to the .json file to load. required Returns: Type Description Dict Content of the .json file as a dictionary. Source code in pydebiaseddta\\utils.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_json ( path : str ) -> Dict : \"\"\"Loads a json file into a dictionary. Parameters ---------- path : str Path to the .json file to load. Returns ------- Dict Content of the .json file as a dictionary. \"\"\" with open ( path , \"r\" ) as f : return json . load ( f )","title":"load_json()"},{"location":"api/utils/#pydebiaseddta.utils.load_sample_dta_data","text":"Loads a portion of the BDB dataset for fast experimenting. Parameters: Name Type Description Default mini bool , optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to True for fast prototyping and False to quickly train a model. Defaults to False . False Returns: Type Description Dict [ str , List ] The dictionary has three keys: \"train\" , \"val\" , and \"test\" , each corresponding to different folds of the dataset. Each key maps to a list with three elements: list of chemicals , list of proteins , and list of affinity scores . The elements in the same index of the lists correspond to a drug-target affinity measurement. Source code in pydebiaseddta\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_sample_dta_data ( mini : bool = False ) -> Dict [ str , List ]: \"\"\"Loads a portion of [the BDB dataset](https://arxiv.org/pdf/2107.05556.pdf) for fast experimenting. Parameters ---------- mini : bool, optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to `True` for fast prototyping and `False` to quickly train a model. Defaults to `False`. Returns ------- Dict[str, List] The dictionary has three keys: `\"train\"`, `\"val\"`, and `\"test\"`, each corresponding to different folds of the dataset. Each key maps to a list with three elements: *list of chemicals*, *list of proteins*, and *list of affinity scores*. The elements in the same index of the lists correspond to a drug-target affinity measurement. \"\"\" sample_data_path = f \" { package_path } /data/dta_sample_data/dta_sample_data.json\" if mini : sample_data_path = f \" { package_path } /data/dta/dta_sample_data.mini.json\" with open ( sample_data_path ) as f : return json . load ( f )","title":"load_sample_dta_data()"},{"location":"api/utils/#pydebiaseddta.utils.load_sample_smiles","text":"Returns examples SMILES strings from ChEMBL for testing. Returns: Type Description List [ str ] SMILES examples from ChEMBL. Source code in pydebiaseddta\\utils.py 30 31 32 33 34 35 36 37 38 39 40 def load_sample_smiles () -> List [ str ]: \"\"\"Returns examples SMILES strings from ChEMBL for testing. Returns ------- List[str] SMILES examples from ChEMBL. \"\"\" sample_data_path = f \" { package_path } /data/sequence/chembl27.mini.smiles\" with open ( sample_data_path ) as f : return [ line . strip () for line in f . readlines ()]","title":"load_sample_smiles()"},{"location":"api/utils/#pydebiaseddta.utils.save_json","text":"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters: Name Type Description Default obj Dict Dictionary to store. required path str Path to store the .json file. required Returns: Type Description None Source code in pydebiaseddta\\utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json ( obj : Dict , path : str ) -> None : \"\"\"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters ---------- obj : Dict Dictionary to store. path : str Path to store the .json file. Returns ------- None \"\"\" with open ( path , \"w\" ) as f : json . dump ( obj , f , indent = 4 )","title":"save_json()"}]}