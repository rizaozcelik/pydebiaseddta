{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pydebiaseddta The API documentation for pydebiaseddta , a python library to improve the generalizability of drug-target affinity (DTA) prediction models. The documentation on this website is continuously updated to further ease the use of DebiasedDTA. Installation Instructions conda create --name pydebiaseddta python=3.9.7 conda activate pydebiaseddta python3 -m pip install pydebiaseddta Citing If you use pydebiaseddta in your research, please cite: @article{ozccelik2022debiaseddta, title={DebiasedDTA: DebiasedDTA: Improving the Generalizability of Drug-Target Affinity Prediction Models}, author={{\\\"O}z{\\c{c}}elik, R{\\i}za and Ba{\\u{g}}, Alperen and At{\\i}l, Berk and Barsbey, Melih and {\\\"O}zg{\\\"u}r, Arzucan and {\\\"O}zk{\\i}r{\\i}ml{\\i}, Elif}, journal={arXiv preprint arXiv:2107.05556}, year={2022} }","title":"Homepage"},{"location":"#pydebiaseddta","text":"The API documentation for pydebiaseddta , a python library to improve the generalizability of drug-target affinity (DTA) prediction models. The documentation on this website is continuously updated to further ease the use of DebiasedDTA.","title":"pydebiaseddta"},{"location":"#installation-instructions","text":"conda create --name pydebiaseddta python=3.9.7 conda activate pydebiaseddta python3 -m pip install pydebiaseddta","title":"Installation Instructions"},{"location":"#citing","text":"If you use pydebiaseddta in your research, please cite: @article{ozccelik2022debiaseddta, title={DebiasedDTA: DebiasedDTA: Improving the Generalizability of Drug-Target Affinity Prediction Models}, author={{\\\"O}z{\\c{c}}elik, R{\\i}za and Ba{\\u{g}}, Alperen and At{\\i}l, Berk and Barsbey, Melih and {\\\"O}zg{\\\"u}r, Arzucan and {\\\"O}zk{\\i}r{\\i}ml{\\i}, Elif}, journal={arXiv preprint arXiv:2107.05556}, year={2022} }","title":"Citing"},{"location":"api/debiasing/","text":"debiasing DebiasedDTA Source code in pydebiaseddta\\debiasing\\debiaseddta.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class DebiasedDTA : def __init__ ( self , guide_cls , predictor_cls , mini_val_frac = 0.2 , n_bootstrapping = 10 , guide_params = None , predictor_params = None , ): self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = {} if guide_params is None else guide_params self . predictor_params = ( {} if predictor_params is None else predictor_params ) self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' ) @staticmethod def save_importance_coefficients ( interactions , importance_coefficients , savedir ): dump_content = [] for interaction_id , chemical , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { chemical } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump ) def learn_importance_coefficients ( self , train_chemicals , train_proteins , train_labels , savedir = None ): train_size = len ( train_chemicals ) train_interactions = list ( zip ( range ( train_size ), train_chemicals , train_proteins , train_labels , ) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for i in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) assert ( len ( mini_train_interactions ) + len ( mini_val_interactions ) == train_size ) mini_train_chemicals = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_chemicals , mini_train_proteins , mini_train_labels , ) mini_val_chemicals = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_chemicals , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds ) ) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) for ix , sq_diffs in enumerate ( interaction_id_to_sq_diff ): assert len ( sq_diffs ) == self . n_bootstrapping interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients def train ( self , train_chemicals , train_proteins , train_labels , val_chemicals = None , val_proteins = None , val_labels = None , coeffs_save_path = None , ): \"\"\"Train function for DebiasedDTA. Parameters ---------- train_chemicals : _type_ _description_ train_proteins : _type_ _description_ train_labels : _type_ _description_ val_chemicals : _type_, optional _description_, by default None val_proteins : _type_, optional _description_, by default None val_labels : _type_, optional _description_, by default None coeffs_save_path : _type_, optional _description_, by default None Returns ------- _type_ _description_ Raises ------ ValueError _description_ \"\"\" train_chemicals = train_chemicals . copy () train_proteins = train_proteins . copy () if len ( train_chemicals ) != len ( train_proteins ): raise ValueError ( \"The number of training chemicals and proteins are different\" ) importance_coefficients = self . learn_importance_coefficients ( train_chemicals , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_chemicals is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , val_chemicals = val_chemicals , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , ) train ( train_chemicals , train_proteins , train_labels , val_chemicals = None , val_proteins = None , val_labels = None , coeffs_save_path = None ) Train function for DebiasedDTA. Parameters: Name Type Description Default train_chemicals _type_ description required train_proteins _type_ description required train_labels _type_ description required val_chemicals _type_ , optional description , by default None None val_proteins _type_ , optional description , by default None None val_labels _type_ , optional description , by default None None coeffs_save_path _type_ , optional description , by default None None Returns: Type Description _type_ description Raises: Type Description ValueError description Source code in pydebiaseddta\\debiasing\\debiaseddta.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def train ( self , train_chemicals , train_proteins , train_labels , val_chemicals = None , val_proteins = None , val_labels = None , coeffs_save_path = None , ): \"\"\"Train function for DebiasedDTA. Parameters ---------- train_chemicals : _type_ _description_ train_proteins : _type_ _description_ train_labels : _type_ _description_ val_chemicals : _type_, optional _description_, by default None val_proteins : _type_, optional _description_, by default None val_labels : _type_, optional _description_, by default None coeffs_save_path : _type_, optional _description_, by default None Returns ------- _type_ _description_ Raises ------ ValueError _description_ \"\"\" train_chemicals = train_chemicals . copy () train_proteins = train_proteins . copy () if len ( train_chemicals ) != len ( train_proteins ): raise ValueError ( \"The number of training chemicals and proteins are different\" ) importance_coefficients = self . learn_importance_coefficients ( train_chemicals , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_chemicals is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , val_chemicals = val_chemicals , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"debiasing"},{"location":"api/debiasing/#debiasing","text":"","title":"debiasing"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA","text":"Source code in pydebiaseddta\\debiasing\\debiaseddta.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 class DebiasedDTA : def __init__ ( self , guide_cls , predictor_cls , mini_val_frac = 0.2 , n_bootstrapping = 10 , guide_params = None , predictor_params = None , ): self . guide_cls = guide_cls self . predictor_cls = predictor_cls self . mini_val_frac = mini_val_frac self . n_bootstrapping = n_bootstrapping self . guide_params = {} if guide_params is None else guide_params self . predictor_params = ( {} if predictor_params is None else predictor_params ) self . predictor_instance = self . predictor_cls ( ** self . predictor_params ) if \"n_epochs\" not in self . predictor_instance . __dict__ : raise ValueError ( 'The predictor must have a field named \"n_epochs\" to be debiased' ) @staticmethod def save_importance_coefficients ( interactions , importance_coefficients , savedir ): dump_content = [] for interaction_id , chemical , protein , label in interactions : importance_coefficient = importance_coefficients [ interaction_id ] dump_content . append ( f \" { chemical } , { protein } , { label } , { importance_coefficient } \" ) dump = \" \\n \" . join ( dump_content ) with open ( savedir ) as f : f . write ( dump ) def learn_importance_coefficients ( self , train_chemicals , train_proteins , train_labels , savedir = None ): train_size = len ( train_chemicals ) train_interactions = list ( zip ( range ( train_size ), train_chemicals , train_proteins , train_labels , ) ) mini_val_data_size = int ( train_size * self . mini_val_frac ) + 1 interaction_id_to_sq_diff = [[] for _ in range ( train_size )] for i in range ( self . n_bootstrapping ): random . shuffle ( train_interactions ) n_mini_val = int ( 1 / self . mini_val_frac ) for mini_val_ix in range ( n_mini_val ): val_start_ix = mini_val_ix * mini_val_data_size val_end_ix = val_start_ix + mini_val_data_size mini_val_interactions = train_interactions [ val_start_ix : val_end_ix ] mini_train_interactions = ( train_interactions [: val_start_ix ] + train_interactions [ val_end_ix :] ) assert ( len ( mini_train_interactions ) + len ( mini_val_interactions ) == train_size ) mini_train_chemicals = [ interaction [ 1 ] for interaction in mini_train_interactions ] mini_train_proteins = [ interaction [ 2 ] for interaction in mini_train_interactions ] mini_train_labels = [ interaction [ 3 ] for interaction in mini_train_interactions ] guide_instance = self . guide_cls ( ** self . guide_params ) guide_instance . train ( mini_train_chemicals , mini_train_proteins , mini_train_labels , ) mini_val_chemicals = [ interaction [ 1 ] for interaction in mini_val_interactions ] mini_val_proteins = [ interaction [ 2 ] for interaction in mini_val_interactions ] preds = guide_instance . predict ( mini_val_chemicals , mini_val_proteins ) mini_val_labels = [ interaction [ 3 ] for interaction in mini_val_interactions ] mini_val_sq_diffs = ( np . array ( mini_val_labels ) - np . array ( preds ) ) ** 2 mini_val_interaction_ids = [ interaction [ 0 ] for interaction in mini_val_interactions ] for interaction_id , sq_diff in zip ( mini_val_interaction_ids , mini_val_sq_diffs ): interaction_id_to_sq_diff [ interaction_id ] . append ( sq_diff ) for ix , sq_diffs in enumerate ( interaction_id_to_sq_diff ): assert len ( sq_diffs ) == self . n_bootstrapping interaction_id_to_med_diff = [ np . median ( diffs ) for diffs in interaction_id_to_sq_diff ] importance_coefficients = [ med / sum ( interaction_id_to_med_diff ) for med in interaction_id_to_med_diff ] if savedir is not None : DebiasedDTA . save_importance_coefficients ( train_interactions , importance_coefficients , savedir ) return importance_coefficients def train ( self , train_chemicals , train_proteins , train_labels , val_chemicals = None , val_proteins = None , val_labels = None , coeffs_save_path = None , ): \"\"\"Train function for DebiasedDTA. Parameters ---------- train_chemicals : _type_ _description_ train_proteins : _type_ _description_ train_labels : _type_ _description_ val_chemicals : _type_, optional _description_, by default None val_proteins : _type_, optional _description_, by default None val_labels : _type_, optional _description_, by default None coeffs_save_path : _type_, optional _description_, by default None Returns ------- _type_ _description_ Raises ------ ValueError _description_ \"\"\" train_chemicals = train_chemicals . copy () train_proteins = train_proteins . copy () if len ( train_chemicals ) != len ( train_proteins ): raise ValueError ( \"The number of training chemicals and proteins are different\" ) importance_coefficients = self . learn_importance_coefficients ( train_chemicals , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_chemicals is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , val_chemicals = val_chemicals , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"DebiasedDTA"},{"location":"api/debiasing/#pydebiaseddta.debiasing.debiaseddta.DebiasedDTA.train","text":"Train function for DebiasedDTA. Parameters: Name Type Description Default train_chemicals _type_ description required train_proteins _type_ description required train_labels _type_ description required val_chemicals _type_ , optional description , by default None None val_proteins _type_ , optional description , by default None None val_labels _type_ , optional description , by default None None coeffs_save_path _type_ , optional description , by default None None Returns: Type Description _type_ description Raises: Type Description ValueError description Source code in pydebiaseddta\\debiasing\\debiaseddta.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def train ( self , train_chemicals , train_proteins , train_labels , val_chemicals = None , val_proteins = None , val_labels = None , coeffs_save_path = None , ): \"\"\"Train function for DebiasedDTA. Parameters ---------- train_chemicals : _type_ _description_ train_proteins : _type_ _description_ train_labels : _type_ _description_ val_chemicals : _type_, optional _description_, by default None val_proteins : _type_, optional _description_, by default None val_labels : _type_, optional _description_, by default None coeffs_save_path : _type_, optional _description_, by default None Returns ------- _type_ _description_ Raises ------ ValueError _description_ \"\"\" train_chemicals = train_chemicals . copy () train_proteins = train_proteins . copy () if len ( train_chemicals ) != len ( train_proteins ): raise ValueError ( \"The number of training chemicals and proteins are different\" ) importance_coefficients = self . learn_importance_coefficients ( train_chemicals , train_proteins , train_labels , savedir = coeffs_save_path , ) n_epochs = self . predictor_instance . n_epochs ic = np . array ( importance_coefficients ) weights_by_epoch = [ 1 - ( e / n_epochs ) + ic * ( e / n_epochs ) for e in range ( n_epochs ) ] if ( val_chemicals is not None and val_proteins is not None and val_labels is not None ): return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , val_chemicals = val_chemicals , val_proteins = val_proteins , val_labels = val_labels , sample_weights_by_epoch = weights_by_epoch , ) return self . predictor_instance . train ( train_chemicals , train_proteins , train_labels , sample_weights_by_epoch = weights_by_epoch , )","title":"train()"},{"location":"api/evaluation/","text":"evaluation ci ( gold_truths , predictions ) Computes concordance index (CI) between the expected values and predictions. See G\u00f6nen and Heller (2005) for the details of the metric. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Concordance index. Source code in pydebiaseddta\\evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ci ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes concordance index (CI) between the expected values and predictions. See [G\u00f6nen and Heller (2005)](https://www.jstor.org/stable/20441249) for the details of the metric. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Concordance index. \"\"\" gold_combs , pred_combs = combinations ( gold_truths , 2 ), combinations ( predictions , 2 ) nominator , denominator = 0 , 0 for ( g1 , g2 ), ( p1 , p2 ) in zip ( gold_combs , pred_combs ): if g2 > g1 : nominator = nominator + 1 * ( p2 > p1 ) + 0.5 * ( p2 == p1 ) denominator = denominator + 1 return float ( nominator / denominator ) evaluate_predictions ( gold_truths , predictions , metrics = None ) Computes multiple metrics with a single call for convenience. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required metrics List [ str ] Name of the evaluation metrics to compute. Possible values are: {\"ci\", \"r2\", \"rmse\", \"mse\"} . All metrics are computed if no value is provided. None Returns: Type Description Dict [ str , float ] A dictionary that maps each metric name to the computed value. Source code in pydebiaseddta\\evaluation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_predictions ( gold_truths : List [ float ], predictions : List [ float ], metrics : List [ str ] = None ) -> Dict [ str , float ]: \"\"\"Computes multiple metrics with a single call for convenience. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. metrics : List[str] Name of the evaluation metrics to compute. Possible values are: `{\"ci\", \"r2\", \"rmse\", \"mse\"}`. All metrics are computed if no value is provided. Returns ------- Dict[str,float] A dictionary that maps each metric name to the computed value. \"\"\" if metrics is None : metrics = [ \"ci\" , \"r2\" , \"rmse\" , \"mse\" ] metrics = [ metric . lower () for metric in metrics ] name_to_fn = { \"ci\" : ci , \"r2\" : r2 , \"rmse\" : rmse , \"mse\" : mse } return { metric : name_to_fn [ metric ]( gold_truths , predictions ) for metric in metrics } mse ( gold_truths , predictions ) Computes mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Mean squared error. Source code in pydebiaseddta\\evaluation.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def mse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = True )) r2 ( gold_truths , predictions ) Compute \\(R^2\\) (coefficient of determinant) between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float \\(R^2\\) (coefficient of determinant) score. Source code in pydebiaseddta\\evaluation.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def r2 ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Compute $R^2$ (coefficient of determinant) between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float $R^2$ (coefficient of determinant) score. \"\"\" return float ( r2_score ( gold_truths , predictions )) rmse ( gold_truths , predictions ) Computes root mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Root mean squared error. Source code in pydebiaseddta\\evaluation.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def rmse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes root mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Root mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = False ))","title":"evaluation"},{"location":"api/evaluation/#evaluation","text":"","title":"evaluation"},{"location":"api/evaluation/#pydebiaseddta.evaluation.ci","text":"Computes concordance index (CI) between the expected values and predictions. See G\u00f6nen and Heller (2005) for the details of the metric. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Concordance index. Source code in pydebiaseddta\\evaluation.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def ci ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes concordance index (CI) between the expected values and predictions. See [G\u00f6nen and Heller (2005)](https://www.jstor.org/stable/20441249) for the details of the metric. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Concordance index. \"\"\" gold_combs , pred_combs = combinations ( gold_truths , 2 ), combinations ( predictions , 2 ) nominator , denominator = 0 , 0 for ( g1 , g2 ), ( p1 , p2 ) in zip ( gold_combs , pred_combs ): if g2 > g1 : nominator = nominator + 1 * ( p2 > p1 ) + 0.5 * ( p2 == p1 ) denominator = denominator + 1 return float ( nominator / denominator )","title":"ci()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.evaluate_predictions","text":"Computes multiple metrics with a single call for convenience. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required metrics List [ str ] Name of the evaluation metrics to compute. Possible values are: {\"ci\", \"r2\", \"rmse\", \"mse\"} . All metrics are computed if no value is provided. None Returns: Type Description Dict [ str , float ] A dictionary that maps each metric name to the computed value. Source code in pydebiaseddta\\evaluation.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def evaluate_predictions ( gold_truths : List [ float ], predictions : List [ float ], metrics : List [ str ] = None ) -> Dict [ str , float ]: \"\"\"Computes multiple metrics with a single call for convenience. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. metrics : List[str] Name of the evaluation metrics to compute. Possible values are: `{\"ci\", \"r2\", \"rmse\", \"mse\"}`. All metrics are computed if no value is provided. Returns ------- Dict[str,float] A dictionary that maps each metric name to the computed value. \"\"\" if metrics is None : metrics = [ \"ci\" , \"r2\" , \"rmse\" , \"mse\" ] metrics = [ metric . lower () for metric in metrics ] name_to_fn = { \"ci\" : ci , \"r2\" : r2 , \"rmse\" : rmse , \"mse\" : mse } return { metric : name_to_fn [ metric ]( gold_truths , predictions ) for metric in metrics }","title":"evaluate_predictions()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.mse","text":"Computes mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Mean squared error. Source code in pydebiaseddta\\evaluation.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def mse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = True ))","title":"mse()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.r2","text":"Compute \\(R^2\\) (coefficient of determinant) between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float \\(R^2\\) (coefficient of determinant) score. Source code in pydebiaseddta\\evaluation.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def r2 ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Compute $R^2$ (coefficient of determinant) between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float $R^2$ (coefficient of determinant) score. \"\"\" return float ( r2_score ( gold_truths , predictions ))","title":"r2()"},{"location":"api/evaluation/#pydebiaseddta.evaluation.rmse","text":"Computes root mean squared error between expected and predicted values. Parameters: Name Type Description Default gold_truths List [ float ] The gold labels in the dataset. required predictions List [ float ] Predictions of a model. required Returns: Type Description float Root mean squared error. Source code in pydebiaseddta\\evaluation.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def rmse ( gold_truths : List [ float ], predictions : List [ float ]) -> float : \"\"\"Computes root mean squared error between expected and predicted values. Parameters ---------- gold_truths : List[float] The gold labels in the dataset. predictions : List[float] Predictions of a model. Returns ------- float Root mean squared error. \"\"\" return float ( mean_squared_error ( gold_truths , predictions , squared = False ))","title":"rmse()"},{"location":"api/guides/","text":"guides Docstring","title":"guides"},{"location":"api/guides/#guides","text":"Docstring","title":"guides"},{"location":"api/predictors/","text":"predictors The submodule that contains the predictors, i.e. , drug-target affinity (DTA) prediction models, implemented in DebiasedDTA study. The implemented predictors are BPEDTA, DeepDTA, and LMDTA. Abstract classes are also available to quickly train a custom DTA prediction model with DebiasedDTA. Predictor Bases: ABC An abstract class that implements the interface of a predictor in pydebiaseddta . The predictors are characterized by an n_epochs attribute and a train function, whose signatures are implemented by this class. Any instance of Predictor class can be trained in the DebiasedDTA training framework, and therefore, Predictor can be inherited to debias custom DTA prediction models. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Predictor ( ABC ): \"\"\"An abstract class that implements the interface of a predictor in `pydebiaseddta`. The predictors are characterized by an `n_epochs` attribute and a `train` function, whose signatures are implemented by this class. Any instance of `Predictor` class can be trained in the `DebiasedDTA` training framework, and therefore, `Predictor` can be inherited to debias custom DTA prediction models. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass __init__ ( n_epochs , * args , ** kwargs ) abstractmethod An abstract constructor for Predictor to display that n_epochs is a necessary attribute for children classes. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 41 42 43 44 45 46 47 48 49 50 @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs train ( train_ligands , train_proteins , train_labels , val_ligands = None , val_proteins = None , val_labels = None , sample_weights_by_epoch = None ) abstractmethod An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters: Name Type Description Default train_ligands List [ Any ] The training ligands as a List. required train_proteins List [ Any ] The training proteins as a List. required train_labels List [ float ] Affinity scores of the training protein-compound pairs required val_ligands List [ Any ], optional Validation ligands as a List, in case validation scores are measured during training, by default None None val_proteins List [ Any ], optional Validation proteins as a List, in case validation scores are measured during training, by default None None val_labels List [ float ], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default None None Returns: Type Description Any The function is free to return any value after its training, including None . Source code in pydebiaseddta\\predictors\\abstract_predictors.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass TFPredictor Bases: Predictor The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. TFPredictor class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and __init__ functions. Model training, prediction, and save/load functions are inherited from this class. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class TFPredictor ( Predictor ): \"\"\"The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. `TFPredictor` class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and `__init__` functions. Model training, prediction, and save/load functions are inherited from this class. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 ) __init__ ( n_epochs , learning_rate , batch_size , ** kwargs ) abstractmethod An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the build function. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required learning_rate float The learning rate of the optimization algorithm. required batch_size _type_ Batch size for training. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () build () abstractmethod An abstract function to create the model architecture. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 117 118 119 120 121 122 @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass from_file ( path ) classmethod A utility function to load a TFPredictor instance from disk. All attributes, including the model weights, are loaded. Parameters: Name Type Description Default path str Path to load the prediction model from. required Returns: Type Description TFPredictor The previously saved model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance predict ( ligands , proteins ) Predicts the affinities of a List of protein-ligand pairs via the trained DTA prediction model, i.e. , BPE-DTA, LM-DTA, and BPE-DTA. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinity scores by DTA prediction model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () save ( path ) A utility function to save a TFPredictor instance to the disk. All attributes, including the model weights, are saved. Parameters: Name Type Description Default path str Path to save the predictor. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 ) train ( train_ligands , train_proteins , train_labels , val_ligands = None , val_proteins = None , val_labels = None , sample_weights_by_epoch = None ) The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ str ], optional SMILES strings of the validation ligands, by default None and no validation is used. None val_proteins List [ str ], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. None val_labels List [ float ], optional Affinity scores of the validation pairs, by default None and no validation is used. None sample_weights_by_epoch List [ np . array ], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size \\(E\\) (number of training epochs), in which each element is a np.array of \\(N imes 1\\) , where \\(N\\) is the training set size and each element corresponds to the weight of a training sample. By default None and no weighting is used. None Returns: Type Description Dict Training history. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history vectorize_ligands ( ligands ) abstractmethod An abstract function to vectorize ligands. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 124 125 126 127 128 129 @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass vectorize_proteins ( proteins ) abstractmethod An abstract function to vectorize proteins. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 131 132 133 134 135 136 @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass create_uniform_weights ( n_samples , n_epochs ) Create a lists of weights such that every training instance has the equal weight across all epoch, i.e. , no sample weighting is used. Parameters: Name Type Description Default n_samples int Number of training instances. required n_epochs int Number of epochs to train the model. required Returns: Type Description List [ np . array ] Sample weights across epochs. Each instance has a weight of 1 for all epochs. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def create_uniform_weights ( n_samples : int , n_epochs : int ) -> List [ np . array ]: \"\"\"Create a lists of weights such that every training instance has the equal weight across all epoch, *i.e.*, no sample weighting is used. Parameters ---------- n_samples : int Number of training instances. n_epochs : int Number of epochs to train the model. Returns ------- List[np.array] Sample weights across epochs. Each instance has a weight of 1 for all epochs. \"\"\" return [ np . array ([ 1 ] * n_samples ) for _ in range ( n_epochs )] DeepDTA Bases: TFPredictor Source code in pydebiaseddta\\predictors\\deepdta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DeepDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) __init__ ( max_smi_len = 100 , max_prot_len = 1000 , embedding_dim = 128 , learning_rate = 0.001 , batch_size = 256 , n_epochs = 200 , num_filters = 32 , smi_filter_len = 4 , prot_filter_len = 6 ) Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. 100 max_prot_len int , optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule characters, by default 128. 128 learning_rate float , optional Leaning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\deepdta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) build () Builds a DeepDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\deepdta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta vectorize_ligands ( ligands ) Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. Source code in pydebiaseddta\\predictors\\deepdta.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) vectorize_proteins ( aa_sequences ) Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. Source code in pydebiaseddta\\predictors\\deepdta.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) BPEDTA Bases: TFPredictor Source code in pydebiaseddta\\predictors\\bpedta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class BPEDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) ) __init__ ( max_smi_len = 100 , max_prot_len = 1000 , embedding_dim = 128 , learning_rate = 0.001 , batch_size = 256 , n_epochs = 200 , num_filters = 32 , smi_filter_len = 4 , prot_filter_len = 6 ) Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. 100 max_prot_len int , optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule words, by default 128. 128 learning_rate float , optional Leaning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\bpedta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) build () Builds a BPEDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\bpedta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta vectorize_ligands ( ligands ) Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of chemical words. Source code in pydebiaseddta\\predictors\\bpedta.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) vectorize_proteins ( aa_sequences ) Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of protein words. Source code in pydebiaseddta\\predictors\\bpedta.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"predictors"},{"location":"api/predictors/#predictors","text":"The submodule that contains the predictors, i.e. , drug-target affinity (DTA) prediction models, implemented in DebiasedDTA study. The implemented predictors are BPEDTA, DeepDTA, and LMDTA. Abstract classes are also available to quickly train a custom DTA prediction model with DebiasedDTA.","title":"predictors"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor","text":"Bases: ABC An abstract class that implements the interface of a predictor in pydebiaseddta . The predictors are characterized by an n_epochs attribute and a train function, whose signatures are implemented by this class. Any instance of Predictor class can be trained in the DebiasedDTA training framework, and therefore, Predictor can be inherited to debias custom DTA prediction models. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Predictor ( ABC ): \"\"\"An abstract class that implements the interface of a predictor in `pydebiaseddta`. The predictors are characterized by an `n_epochs` attribute and a `train` function, whose signatures are implemented by this class. Any instance of `Predictor` class can be trained in the `DebiasedDTA` training framework, and therefore, `Predictor` can be inherited to debias custom DTA prediction models. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass","title":"Predictor"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor.__init__","text":"An abstract constructor for Predictor to display that n_epochs is a necessary attribute for children classes. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 41 42 43 44 45 46 47 48 49 50 @abstractmethod def __init__ ( self , n_epochs : int , * args , ** kwargs ) -> None : \"\"\"An abstract constructor for `Predictor` to display that `n_epochs` is a necessary attribute for children classes. Parameters ---------- n_epochs : int Number of epochs to train the model. \"\"\" self . n_epochs = n_epochs","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.Predictor.train","text":"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters: Name Type Description Default train_ligands List [ Any ] The training ligands as a List. required train_proteins List [ Any ] The training proteins as a List. required train_labels List [ float ] Affinity scores of the training protein-compound pairs required val_ligands List [ Any ], optional Validation ligands as a List, in case validation scores are measured during training, by default None None val_proteins List [ Any ], optional Validation proteins as a List, in case validation scores are measured during training, by default None None val_labels List [ float ], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default None None Returns: Type Description Any The function is free to return any value after its training, including None . Source code in pydebiaseddta\\predictors\\abstract_predictors.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @abstractmethod def train ( self , train_ligands : List [ Any ], train_proteins : List [ Any ], train_labels : List [ float ], val_ligands : List [ Any ] = None , val_proteins : List [ Any ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Any : \"\"\"An abstract method to train DTA prediction models. The inputs can be of any biomolecule representation type. However, the training procedure must support sample weighting in every epoch. Parameters ---------- train_ligands : List[Any] The training ligands as a List. train_proteins : List[Any] The training proteins as a List. train_labels : List[float] Affinity scores of the training protein-compound pairs val_ligands : List[Any], optional Validation ligands as a List, in case validation scores are measured during training, by default `None` val_proteins : List[Any], optional Validation proteins as a List, in case validation scores are measured during training, by default `None` val_labels : List[float], optional Affinity scores of validation protein-compound pairs as a List, in case validation scores are measured during training, by default `None` Returns ------- Any The function is free to return any value after its training, including `None`. \"\"\" pass","title":"train()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor","text":"Bases: Predictor The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. TFPredictor class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and __init__ functions. Model training, prediction, and save/load functions are inherited from this class. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 class TFPredictor ( Predictor ): \"\"\"The models in DebiasedDTA study (BPE-DTA, LM-DTA, DeepDTA) are implemented in Tensorflow. `TFPredictor` class provides an abstraction to these models to minimize code duplication. The children classes only implement model building, biomolecule vectorization, and `__init__` functions. Model training, prediction, and save/load functions are inherited from this class. \"\"\" @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build () @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist () def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 )","title":"TFPredictor"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.__init__","text":"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the build function. Parameters: Name Type Description Default n_epochs int Number of epochs to train the model. required learning_rate float The learning rate of the optimization algorithm. required batch_size _type_ Batch size for training. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 @abstractmethod def __init__ ( self , n_epochs : int , learning_rate : float , batch_size : int , ** kwargs ): \"\"\"An abstract constructor for BPE-DTA, LM-DTA, and DeepDTA. The constructor sets the common attributes and call the `build` function. Parameters ---------- n_epochs : int Number of epochs to train the model. learning_rate : float The learning rate of the optimization algorithm. batch_size : _type_ Batch size for training. \"\"\" self . n_epochs = n_epochs self . learning_rate = learning_rate self . batch_size = batch_size self . history = dict () self . model = self . build ()","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.build","text":"An abstract function to create the model architecture. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 117 118 119 120 121 122 @abstractmethod def build ( self ): \"\"\"An abstract function to create the model architecture. Every child has to implement this function. \"\"\" pass","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.from_file","text":"A utility function to load a TFPredictor instance from disk. All attributes, including the model weights, are loaded. Parameters: Name Type Description Default path str Path to load the prediction model from. required Returns: Type Description TFPredictor The previously saved model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 @classmethod def from_file ( cls , path : str ): \"\"\"A utility function to load a `TFPredictor` instance from disk. All attributes, including the model weights, are loaded. Parameters ---------- path : str Path to load the prediction model from. Returns ------- TFPredictor The previously saved model. \"\"\" with open ( f \" { path } /params.json\" ) as f : dct = json . load ( f ) instance = cls ( ** dct ) instance . model = tf . keras . models . load_model ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" ) as f : instance . history = json . load ( f ) return instance","title":"from_file()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.predict","text":"Predicts the affinities of a List of protein-ligand pairs via the trained DTA prediction model, i.e. , BPE-DTA, LM-DTA, and BPE-DTA. Parameters: Name Type Description Default ligands List [ str ] SMILES strings of the ligands. required proteins List [ str ] Amino-acid sequences of the proteins. required Returns: Type Description List [ float ] Predicted affinity scores by DTA prediction model. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def predict ( self , ligands : List [ str ], proteins : List [ str ]) -> List [ float ]: \"\"\"Predicts the affinities of a `List` of protein-ligand pairs via the trained DTA prediction model, *i.e.*, BPE-DTA, LM-DTA, and BPE-DTA. Parameters ---------- ligands : List[str] SMILES strings of the ligands. proteins : List[str] Amino-acid sequences of the proteins. Returns ------- List[float] Predicted affinity scores by DTA prediction model. \"\"\" ligand_vectors = self . vectorize_ligands ( ligands ) protein_vectors = self . vectorize_proteins ( proteins ) return self . model . predict ([ ligand_vectors , protein_vectors ]) . tolist ()","title":"predict()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.save","text":"A utility function to save a TFPredictor instance to the disk. All attributes, including the model weights, are saved. Parameters: Name Type Description Default path str Path to save the predictor. required Source code in pydebiaseddta\\predictors\\abstract_predictors.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def save ( self , path : str ) -> None : \"\"\"A utility function to save a `TFPredictor` instance to the disk. All attributes, including the model weights, are saved. Parameters ---------- path : str Path to save the predictor. \"\"\" self . model . save ( f \" { path } /model\" ) with open ( f \" { path } /history.json\" , \"w\" ) as f : json . dump ( self . history , f , indent = 4 ) donot_copy = { \"model\" , \"history\" } dct = { k : v for k , v in self . __dict__ . items () if k not in donot_copy } with open ( f \" { path } /params.json\" , \"w\" ) as f : json . dump ( dct , f , indent = 4 )","title":"save()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.train","text":"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters: Name Type Description Default train_ligands List [ str ] SMILES strings of the training ligands. required train_proteins List [ str ] Amino-acid sequences of the training proteins. required train_labels List [ float ] Affinity scores of the training protein-ligand pairs. required val_ligands List [ str ], optional SMILES strings of the validation ligands, by default None and no validation is used. None val_proteins List [ str ], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. None val_labels List [ float ], optional Affinity scores of the validation pairs, by default None and no validation is used. None sample_weights_by_epoch List [ np . array ], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size \\(E\\) (number of training epochs), in which each element is a np.array of \\(N imes 1\\) , where \\(N\\) is the training set size and each element corresponds to the weight of a training sample. By default None and no weighting is used. None Returns: Type Description Dict Training history. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def train ( self , train_ligands : List [ str ], train_proteins : List [ str ], train_labels : List [ float ], val_ligands : List [ str ] = None , val_proteins : List [ str ] = None , val_labels : List [ float ] = None , sample_weights_by_epoch : List [ np . array ] = None , ) -> Dict : \"\"\"The common model training procedure for BPE-DTA, LM-DTA, and DeepDTA. The models adopt different biomolecule representation methods and model architectures, so, the training results are different. The training procedure supports validation for tracking, and sample weighting for debiasing. Parameters ---------- train_ligands : List[str] SMILES strings of the training ligands. train_proteins : List[str] Amino-acid sequences of the training proteins. train_labels : List[float] Affinity scores of the training protein-ligand pairs. val_ligands : List[str], optional SMILES strings of the validation ligands, by default None and no validation is used. val_proteins : List[str], optional Amino-acid sequences of the validation proteins, by default None and no validation is used. val_labels : List[float], optional Affinity scores of the validation pairs, by default None and no validation is used. sample_weights_by_epoch : List[np.array], optional Weight of each training protein-ligand pair during training across epochs. This variable must be a List of size $E$ (number of training epochs), in which each element is a `np.array` of $N\\times 1$, where $N$ is the training set size and each element corresponds to the weight of a training sample. By default `None` and no weighting is used. Returns ------- Dict Training history. \"\"\" if sample_weights_by_epoch is None : sample_weights_by_epoch = create_uniform_weights ( len ( train_ligands ), self . n_epochs ) train_ligand_vectors = self . vectorize_ligands ( train_ligands ) train_protein_vectors = self . vectorize_proteins ( train_proteins ) train_labels = np . array ( train_labels ) val_tuple = None if ( val_ligands is not None and val_proteins is not None and val_labels is not None ): val_ligand_vectors = self . vectorize_ligands ( val_ligands ) val_protein_vectors = self . vectorize_proteins ( val_proteins ) val_tuple = ( [ val_ligand_vectors , val_protein_vectors ], np . array ( val_labels ), ) train_stats_over_epochs = { \"mse\" : [], \"rmse\" : [], \"r2\" : []} val_stats_over_epochs = train_stats_over_epochs . copy () for e in range ( self . n_epochs ): self . model . fit ( x = [ train_ligand_vectors , train_protein_vectors ], y = train_labels , sample_weight = sample_weights_by_epoch [ e ], validation_data = val_tuple , batch_size = self . batch_size , epochs = 1 , ) train_stats = evaluate_predictions ( gold_truths = train_labels , predictions = self . predict ( train_ligands , train_proteins ), metrics = list ( train_stats_over_epochs . keys ()), ) for metric , stat in train_stats . items (): train_stats_over_epochs [ metric ] . append ( stat ) if val_tuple is not None : val_stats = evaluate_predictions ( y_true = val_labels , y_preds = self . predict ( val_tuple [ 0 ], val_tuple [ 1 ]), metrics = list ( val_stats_over_epochs . keys ()), ) for metric , stat in val_stats . items (): val_stats_over_epochs [ metric ] . append ( stat ) self . history [ \"train\" ] = train_stats_over_epochs if val_stats_over_epochs is not None : self . history [ \"val\" ] = val_stats_over_epochs return self . history","title":"train()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.vectorize_ligands","text":"An abstract function to vectorize ligands. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 124 125 126 127 128 129 @abstractmethod def vectorize_ligands ( self , ligands ): \"\"\"An abstract function to vectorize ligands. Every child has to implement this function. \"\"\" pass","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.TFPredictor.vectorize_proteins","text":"An abstract function to vectorize proteins. Every child has to implement this function. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 131 132 133 134 135 136 @abstractmethod def vectorize_proteins ( self , proteins ): \"\"\"An abstract function to vectorize proteins. Every child has to implement this function. \"\"\" pass","title":"vectorize_proteins()"},{"location":"api/predictors/#pydebiaseddta.predictors.abstract_predictors.create_uniform_weights","text":"Create a lists of weights such that every training instance has the equal weight across all epoch, i.e. , no sample weighting is used. Parameters: Name Type Description Default n_samples int Number of training instances. required n_epochs int Number of epochs to train the model. required Returns: Type Description List [ np . array ] Sample weights across epochs. Each instance has a weight of 1 for all epochs. Source code in pydebiaseddta\\predictors\\abstract_predictors.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def create_uniform_weights ( n_samples : int , n_epochs : int ) -> List [ np . array ]: \"\"\"Create a lists of weights such that every training instance has the equal weight across all epoch, *i.e.*, no sample weighting is used. Parameters ---------- n_samples : int Number of training instances. n_epochs : int Number of epochs to train the model. Returns ------- List[np.array] Sample weights across epochs. Each instance has a weight of 1 for all epochs. \"\"\" return [ np . array ([ 1 ] * n_samples ) for _ in range ( n_epochs )]","title":"create_uniform_weights()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA","text":"Bases: TFPredictor Source code in pydebiaseddta\\predictors\\deepdta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class DeepDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) ) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"DeepDTA"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.__init__","text":"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. 100 max_prot_len int , optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule characters, by default 128. 128 learning_rate float , optional Leaning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\deepdta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a DeepDTA instance. DeepDTA segments SMILES strings of ligands and amino-acid sequences of proteins into characters, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of characters in a SMILES string, by default 100. Longer SMILES strings are truncated. max_prot_len : int, optional Maximum number of amino-acids a protein sequence, by default 1000. Longer sequences are truncated. embedding_dim : int, optional The dimension of the biomolecule characters, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 94 self . prot_vocab_size = 26 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size )","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.build","text":"Builds a DeepDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\deepdta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def build ( self ): \"\"\"Builds a `DeepDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) deepdta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) deepdta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return deepdta","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.vectorize_ligands","text":"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. Source code in pydebiaseddta\\predictors\\deepdta.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into characters and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of SMILES tokens. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 94 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ) )","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.deepdta.DeepDTA.vectorize_proteins","text":"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. Source code in pydebiaseddta\\predictors\\deepdta.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into characters and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of amino-acids. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 26 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"vectorize_proteins()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA","text":"Bases: TFPredictor Source code in pydebiaseddta\\predictors\\bpedta.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 class BPEDTA ( TFPredictor ): def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size ) def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len )) def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"BPEDTA"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.__init__","text":"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters: Name Type Description Default max_smi_len int , optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. 100 max_prot_len int , optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. 1000 embedding_dim int , optional The dimension of the biomolecule words, by default 128. 128 learning_rate float , optional Leaning rate during optimization, by default 0.001. 0.001 batch_size int , optional Batch size during training, by default 256. 256 n_epochs int , optional Number of epochs to train the model, by default 200. 200 num_filters int , optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. 32 smi_filter_len int , optional Length of filters in the convolution blocks for ligands, by default 4. 4 prot_filter_len int , optional Length of filters in the convolution blocks for proteins, by default 6. 6 Source code in pydebiaseddta\\predictors\\bpedta.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , max_smi_len : int = 100 , max_prot_len : int = 1000 , embedding_dim : int = 128 , learning_rate : float = 0.001 , batch_size : int = 256 , n_epochs : int = 200 , num_filters : int = 32 , smi_filter_len : int = 4 , prot_filter_len : int = 6 , ): \"\"\"Constructor to create a BPE-DTA instance. BPE-DTA segments SMILES strings of ligands and amino-acid sequences of proteins into biomolecule words, and applies three layers of convolutions to learn latent representations. A fully-connected neural network with three layers is used afterwards to predict affinities. Parameters ---------- max_smi_len : int, optional Maximum number of chemical words in a SMILES string, by default 100. SMILES strings that contain more chemical words are truncated. max_prot_len : int, optional Maximum number of protein words in an amino-acid sequence, by default 1000. Amino-acid sequences that contain more proteins words are truncated. embedding_dim : int, optional The dimension of the biomolecule words, by default 128. learning_rate : float, optional Leaning rate during optimization, by default 0.001. batch_size : int, optional Batch size during training, by default 256. n_epochs : int, optional Number of epochs to train the model, by default 200. num_filters : int, optional Number of filters in the first convolution block. The next blocks use two and three times of this number, respectively. y default 32. smi_filter_len : int, optional Length of filters in the convolution blocks for ligands, by default 4. prot_filter_len : int, optional Length of filters in the convolution blocks for proteins, by default 6. \"\"\" self . max_smi_len = max_smi_len self . max_prot_len = max_prot_len self . embedding_dim = embedding_dim self . num_filters = num_filters self . smi_filter_len = smi_filter_len self . prot_filter_len = prot_filter_len self . chem_vocab_size = 8000 self . prot_vocab_size = 32000 TFPredictor . __init__ ( self , n_epochs , learning_rate , batch_size )","title":"__init__()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.build","text":"Builds a BPEDTA predictor in keras with the parameters specified during construction. Returns: Type Description tensorflow . keras . models . Model The built model. Source code in pydebiaseddta\\predictors\\bpedta.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def build ( self ) -> Model : \"\"\"Builds a `BPEDTA` predictor in `keras` with the parameters specified during construction. Returns ------- tensorflow.keras.models.Model The built model. \"\"\" # Inputs ligands = Input ( shape = ( self . max_smi_len ,), dtype = \"int32\" ) # chemical representation ligand_representation = Embedding ( input_dim = self . chem_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_smi_len , mask_zero = True , )( ligands ) ligand_representation = Conv1D ( filters = self . num_filters , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . smi_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( ligand_representation ) ligand_representation = GlobalMaxPooling1D ()( ligand_representation ) # Protein representation proteins = Input ( shape = ( self . max_prot_len ,), dtype = \"int32\" ) protein_representation = Embedding ( input_dim = self . prot_vocab_size + 1 , output_dim = self . embedding_dim , input_length = self . max_prot_len , mask_zero = True , )( proteins ) protein_representation = Conv1D ( filters = self . num_filters , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 2 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = Conv1D ( filters = self . num_filters * 3 , kernel_size = self . prot_filter_len , activation = \"relu\" , padding = \"valid\" , strides = 1 , )( protein_representation ) protein_representation = GlobalMaxPooling1D ()( protein_representation ) interaction_representation = Concatenate ( axis =- 1 )( [ ligand_representation , protein_representation ] ) # Fully connected layers FC1 = Dense ( 1024 , activation = \"relu\" )( interaction_representation ) FC1 = Dropout ( 0.1 )( FC1 ) FC2 = Dense ( 1024 , activation = \"relu\" )( FC1 ) FC2 = Dropout ( 0.1 )( FC2 ) FC3 = Dense ( 512 , activation = \"relu\" )( FC2 ) predictions = Dense ( 1 , kernel_initializer = \"normal\" )( FC3 ) opt = Adam ( self . learning_rate ) bpedta = Model ( inputs = [ ligands , proteins ], outputs = [ predictions ]) bpedta . compile ( optimizer = opt , loss = \"mean_squared_error\" , metrics = [ \"mean_squared_error\" ], ) return bpedta","title":"build()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.vectorize_ligands","text":"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters: Name Type Description Default ligands List [ str ] The SMILES strings of ligands. required Returns: Type Description np . array An \\(N \\times max\\_smi\\_len\\) ( \\(N\\) is the number of the input ligands) matrix that contains label encoded sequences of chemical words. Source code in pydebiaseddta\\predictors\\bpedta.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def vectorize_ligands ( self , ligands : List [ str ]) -> np . array : \"\"\"Segments SMILES strings of ligands into chemical words and applies label encoding. Truncation and padding are also applied to prepare ligands for training and/or prediction. Parameters ---------- ligands : List[str] The SMILES strings of ligands. Returns ------- np.array An $N \\\\times max\\\\_smi\\\\_len$ ($N$ is the number of the input ligands) matrix that contains label encoded sequences of chemical words. \"\"\" smi_to_unichar_encoding = load_smiles_to_unichar_encoding () unichars = smiles_to_unichar_batch ( ligands , smi_to_unichar_encoding ) word_identifier = load_chemical_word_identifier ( vocab_size = 8000 ) return np . array ( word_identifier . encode_sequences ( unichars , self . max_smi_len ))","title":"vectorize_ligands()"},{"location":"api/predictors/#pydebiaseddta.predictors.bpedta.BPEDTA.vectorize_proteins","text":"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters: Name Type Description Default aa_sequences List [ str ] The amino-acid sequences of proteins. required Returns: Type Description np . array An \\(N \\times max\\_prot\\_len\\) ( \\(N\\) is the number of the input proteins) matrix that contains label encoded sequences of protein words. Source code in pydebiaseddta\\predictors\\bpedta.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def vectorize_proteins ( self , aa_sequences : List [ str ]) -> np . array : \"\"\"Segments amino-acid sequences of proteins into protein words and applies label encoding. Truncation and padding are also applied to prepare proteins for training and/or prediction. Parameters ---------- aa_sequences : List[str] The amino-acid sequences of proteins. Returns ------- np.array An $N \\\\times max\\\\_prot\\\\_len$ ($N$ is the number of the input proteins) matrix that contains label encoded sequences of protein words. \"\"\" word_identifier = load_protein_word_identifier ( vocab_size = 32000 ) return np . array ( word_identifier . encode_sequences ( aa_sequences , self . max_prot_len ) )","title":"vectorize_proteins()"},{"location":"api/sequence/","text":"sequence The submodule for processing SMILES strings. smiles_processing.py consists of utility function to segment SMILES strings, whereas word_identification.py consists of a class to learn biomolecule words and segment biomolecule sequences into biomolecule words. segment_smiles ( smiles , segment_sq_brackets = True ) Segments a SMILES string into its tokens. Parameters: Name Type Description Default smiles str Input SMILES string. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets ( e.g. [C@@H], [Rb]), too. Set to True to have square brackets and the tokens inside as standalone tokens, e.g. [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to False , whole expression is returned as a single token, e.g. \"[C@@H]\" . Defaults to True . True Returns: Type Description List [ str ] Each element of the SMILES string as a list. Source code in pydebiaseddta\\sequence\\smiles_processing.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def segment_smiles ( smiles : str , segment_sq_brackets : bool = True ) -> List [ str ]: \"\"\"Segments a SMILES string into its tokens. Parameters ---------- smiles : str Input SMILES string. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets (*e.g.* [C@@H], [Rb]), too. Set to `True` to have square brackets and the tokens inside as standalone tokens, *e.g.* [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to `False`, whole expression is returned as a single token, *e.g.* \"[C@@H]\" . Defaults to `True`. Returns ------- List[str] Each element of the SMILES string as a list. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles ) segment_smiles_batch ( smiles_batch , segment_sq_brackets = True ) Segments multiple SMILES strings with a single call by wrapping sequence.smiles_processing.segment_smiles . Parameters: Name Type Description Default smiles_batch List [ str ] List of input SMILES strings. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets. See sequence.smiles_processing.segment_smiles for a more detailed explanation. Defaults to True . True Returns: Type Description List [ List [ str ]] A 2D list of strings where element \\([i][j]\\) corresponds to the \\(j^{th}\\) token of the \\(i^{th}\\) input. Source code in pydebiaseddta\\sequence\\smiles_processing.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segments multiple SMILES strings with a single call by wrapping `sequence.smiles_processing.segment_smiles`. Parameters ---------- smiles_batch : List[str] List of input SMILES strings. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets. See `sequence.smiles_processing.segment_smiles` for a more detailed explanation. Defaults to `True`. Returns ------- List[List[str]] A 2D list of strings where element $[i][j]$ corresponds to the $j^{th}$ token of the $i^{th}$ input. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ] WordIdentifier A versatile class to identify biomolecule words in biomolecule strings. WordIdentifier leverages the Byte Pair Encoding algorithm implemented in the tokenizers library to learn biomolecule vocabularies and segment biomolecule strings into their words. Source code in pydebiaseddta\\sequence\\word_identification.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class WordIdentifier : \"\"\"A versatile class to identify biomolecule words in biomolecule strings. `WordIdentifier` leverages the Byte Pair Encoding algorithm implemented in the `tokenizers` library to learn biomolecule vocabularies and segment biomolecule strings into their words. \"\"\" def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath ) __init__ ( vocab_size ) Creates a WordIdentifier instance. Parameters: Name Type Description Default vocab_size int Size of the biomolecule vocabulary. required Source code in pydebiaseddta\\sequence\\word_identification.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () encode_sequences ( sequences , padding_len = None ) Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required padding_len int , optional The desired length of sequences, by default None . No padding is applied when set to None . None Returns: Type Description List [ List [ int ]] List of the id of the biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] from_file ( loadpath ) classmethod Loads a WordIdentifier from a file. Parameters: Name Type Description Default loadpath str Path to the WordIdentifier file. required Returns: Type Description WordIdentifier Previously saved WordIdentifier Source code in pydebiaseddta\\sequence\\word_identification.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance save ( savepath ) Saves a WordIdentifier instance to disk. Parameters: Name Type Description Default savepath str The path to dump the instance. File extension is added automatically. required Source code in pydebiaseddta\\sequence\\word_identification.py 114 115 116 117 118 119 120 121 122 123 124 def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath ) tokenize_sequences ( sequences ) Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required Returns: Type Description List [ List [ str ]] List of biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] train ( corpus_path ) Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters: Name Type Description Default corpus_path str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. required Source code in pydebiaseddta\\sequence\\word_identification.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) load_chemical_word_identifier ( vocab_size ) A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 94 and 8000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def load_chemical_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 94 and 8000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 94 , 8000 ]: raise ValueError ( \"Supported vocab sizes are 94 and 8000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/chemical\" vocab_path = f \" { protein_vocab_path } /chembl27_enc_94.json\" if vocab_size == 8000 : vocab_path = f \" { protein_vocab_path } /chembl27_enc_bpe_8000.json\" return WordIdentifier . from_file ( vocab_path ) load_protein_word_identifier ( vocab_size ) A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 26 and 32000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def load_protein_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 26 and 32000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 26 , 32000 ]: raise ValueError ( \"Supported vocab sizes are 26 and 32000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/protein\" vocab_path = f \" { protein_vocab_path } /uniprot_26.json\" if vocab_size == 32000 : vocab_path = f \" { protein_vocab_path } /uniprot_bpe_32000.json\" return WordIdentifier . from_file ( vocab_path )","title":"sequence"},{"location":"api/sequence/#sequence","text":"The submodule for processing SMILES strings. smiles_processing.py consists of utility function to segment SMILES strings, whereas word_identification.py consists of a class to learn biomolecule words and segment biomolecule sequences into biomolecule words.","title":"sequence"},{"location":"api/sequence/#pydebiaseddta.sequence.smiles_processing.segment_smiles","text":"Segments a SMILES string into its tokens. Parameters: Name Type Description Default smiles str Input SMILES string. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets ( e.g. [C@@H], [Rb]), too. Set to True to have square brackets and the tokens inside as standalone tokens, e.g. [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to False , whole expression is returned as a single token, e.g. \"[C@@H]\" . Defaults to True . True Returns: Type Description List [ str ] Each element of the SMILES string as a list. Source code in pydebiaseddta\\sequence\\smiles_processing.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def segment_smiles ( smiles : str , segment_sq_brackets : bool = True ) -> List [ str ]: \"\"\"Segments a SMILES string into its tokens. Parameters ---------- smiles : str Input SMILES string. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets (*e.g.* [C@@H], [Rb]), too. Set to `True` to have square brackets and the tokens inside as standalone tokens, *e.g.* [\"[\", \"C\", \"@\", \"@\", \"H\", \"]\"]. When set to `False`, whole expression is returned as a single token, *e.g.* \"[C@@H]\" . Defaults to `True`. Returns ------- List[str] Each element of the SMILES string as a list. \"\"\" regex = _RE_PATTERNS [ \"segmentation_sq\" ] if not segment_sq_brackets : regex = _RE_PATTERNS [ \"segmentation\" ] return regex . findall ( smiles )","title":"segment_smiles()"},{"location":"api/sequence/#pydebiaseddta.sequence.smiles_processing.segment_smiles_batch","text":"Segments multiple SMILES strings with a single call by wrapping sequence.smiles_processing.segment_smiles . Parameters: Name Type Description Default smiles_batch List [ str ] List of input SMILES strings. required segment_sq_brackets bool , optional Whether to segment expressions within square brackets. See sequence.smiles_processing.segment_smiles for a more detailed explanation. Defaults to True . True Returns: Type Description List [ List [ str ]] A 2D list of strings where element \\([i][j]\\) corresponds to the \\(j^{th}\\) token of the \\(i^{th}\\) input. Source code in pydebiaseddta\\sequence\\smiles_processing.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def segment_smiles_batch ( smiles_batch : List [ str ], segment_sq_brackets = True ) -> List [ List [ str ]]: \"\"\"Segments multiple SMILES strings with a single call by wrapping `sequence.smiles_processing.segment_smiles`. Parameters ---------- smiles_batch : List[str] List of input SMILES strings. segment_sq_brackets : bool, optional Whether to segment expressions within square brackets. See `sequence.smiles_processing.segment_smiles` for a more detailed explanation. Defaults to `True`. Returns ------- List[List[str]] A 2D list of strings where element $[i][j]$ corresponds to the $j^{th}$ token of the $i^{th}$ input. \"\"\" return [ segment_smiles ( smiles , segment_sq_brackets ) for smiles in smiles_batch ]","title":"segment_smiles_batch()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier","text":"A versatile class to identify biomolecule words in biomolecule strings. WordIdentifier leverages the Byte Pair Encoding algorithm implemented in the tokenizers library to learn biomolecule vocabularies and segment biomolecule strings into their words. Source code in pydebiaseddta\\sequence\\word_identification.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class WordIdentifier : \"\"\"A versatile class to identify biomolecule words in biomolecule strings. `WordIdentifier` leverages the Byte Pair Encoding algorithm implemented in the `tokenizers` library to learn biomolecule vocabularies and segment biomolecule strings into their words. \"\"\" def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace () @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" ) def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ] def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ] def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath )","title":"WordIdentifier"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.__init__","text":"Creates a WordIdentifier instance. Parameters: Name Type Description Default vocab_size int Size of the biomolecule vocabulary. required Source code in pydebiaseddta\\sequence\\word_identification.py 19 20 21 22 23 24 25 26 27 28 29 def __init__ ( self , vocab_size : int ): \"\"\"Creates a `WordIdentifier` instance. Parameters ---------- vocab_size : int Size of the biomolecule vocabulary. \"\"\" self . vocab_size = vocab_size self . tokenizer = Tokenizer ( BPE ()) self . tokenizer . pre_tokenizer = Whitespace ()","title":"__init__()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.encode_sequences","text":"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required padding_len int , optional The desired length of sequences, by default None . No padding is applied when set to None . None Returns: Type Description List [ List [ int ]] List of the id of the biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def encode_sequences ( self , sequences : List [ str ], padding_len : int = None ) -> List [ List [ int ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary and returns the id of the biomolecule word, which is convenient to apply label encoding in the subsequent steps. Padding support is also available to ease training deep learning possible. Parameters ---------- sequences : List[str] The List of biomolecule strings. padding_len : int, optional The desired length of sequences, by default `None`. No padding is applied when set to `None`. Returns ------- List[List[int]] List of the id of the biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) if isinstance ( padding_len , int ): for encoding in encodings : encoding . pad ( padding_len , direction = \"right\" , pad_id = 0 , pad_token = \"[PAD]\" ) encoding . truncate ( padding_len ) return [ encoding . ids for encoding in encodings ]","title":"encode_sequences()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.from_file","text":"Loads a WordIdentifier from a file. Parameters: Name Type Description Default loadpath str Path to the WordIdentifier file. required Returns: Type Description WordIdentifier Previously saved WordIdentifier Source code in pydebiaseddta\\sequence\\word_identification.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @classmethod def from_file ( cls , loadpath : str ): \"\"\"Loads a `WordIdentifier` from a file. Parameters ---------- loadpath : str Path to the `WordIdentifier` file. Returns ------- WordIdentifier Previously saved `WordIdentifier` \"\"\" if not loadpath . endswith ( FILE_EXTENSION ): loadpath = loadpath + FILE_EXTENSION dct = load_json ( loadpath ) vocab_size = len ( dct [ \"model\" ][ \"vocab\" ]) instance = cls ( vocab_size ) instance . tokenizer = Tokenizer . from_str ( json . dumps ( dct )) return instance","title":"from_file()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.save","text":"Saves a WordIdentifier instance to disk. Parameters: Name Type Description Default savepath str The path to dump the instance. File extension is added automatically. required Source code in pydebiaseddta\\sequence\\word_identification.py 114 115 116 117 118 119 120 121 122 123 124 def save ( self , savepath : str ): \"\"\"Saves a `WordIdentifier` instance to disk. Parameters ---------- savepath : str The path to dump the instance. File extension is added automatically. \"\"\" if not savepath . endswith ( FILE_EXTENSION ): savepath = savepath + FILE_EXTENSION save_json ( json . loads ( self . tokenizer . to_str ()), savepath )","title":"save()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.tokenize_sequences","text":"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters: Name Type Description Default sequences List [ str ] The List of biomolecule strings. required Returns: Type Description List [ List [ str ]] List of biomolecule words of each input string. Source code in pydebiaseddta\\sequence\\word_identification.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def tokenize_sequences ( self , sequences : List [ str ]) -> List [ List [ str ]]: \"\"\"Segments a List of biomolecule strings into biomolecule words via the learned vocabulary. Parameters ---------- sequences : List[str] The List of biomolecule strings. Returns ------- List[List[str]] List of biomolecule words of each input string. \"\"\" encodings = self . tokenizer . encode_batch ( sequences ) return [ encoding . tokens for encoding in encodings ]","title":"tokenize_sequences()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.WordIdentifier.train","text":"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters: Name Type Description Default corpus_path str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. required Source code in pydebiaseddta\\sequence\\word_identification.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def train ( self , corpus_path : str ): \"\"\"Learns a biomolecule vocabulary from a file of biomolecule strings using Byte Pair Encoding Algorithm. Parameters ---------- corpus_path : str Path to the corpus of biomolecule strings. The corpus file must contain a biomolecule string per line. \"\"\" trainer = BpeTrainer ( vocab_size = self . vocab_size , special_tokens = [ \"[PAD]\" ] ) self . tokenizer . train ([ corpus_path ], trainer ) if self . tokenizer . get_vocab_size () < self . vocab_size : print ( f \"Warning: The iterations stopped before the desired vocab size is reached. Learned vocab size= { self . tokenizer . get_vocab_size () } . Desired size= { self . vocab_size } \" )","title":"train()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.load_chemical_word_identifier","text":"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 94 and 8000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def load_chemical_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for SMILES strings in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 94 and 8000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 94 and 8000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 94 , 8000 ]: raise ValueError ( \"Supported vocab sizes are 94 and 8000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/chemical\" vocab_path = f \" { protein_vocab_path } /chembl27_enc_94.json\" if vocab_size == 8000 : vocab_path = f \" { protein_vocab_path } /chembl27_enc_bpe_8000.json\" return WordIdentifier . from_file ( vocab_path )","title":"load_chemical_word_identifier()"},{"location":"api/sequence/#pydebiaseddta.sequence.word_identification.load_protein_word_identifier","text":"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters: Name Type Description Default vocab_size int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. required Returns: Type Description type [ WordIdentifier ] The WordIdentifier instance used by the DTA models. Raises: Type Description ValueError If vocabulary size besides 26 and 32000 is passed, a ValueError is raised. Source code in pydebiaseddta\\sequence\\word_identification.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def load_protein_word_identifier ( vocab_size : int ) -> WordIdentifier : \"\"\"A convenience function to load word vocabularies learned for amino-acid sequences in the study. The possible vocabularies to load are for DeepDTA and BPE-DTA. Parameters ---------- vocab_size : int Size of the learned SMILES word vocabulary. The allowed values are 26 and 32000, for DeepDTA and BPE-DTA, respectively. Returns ------- type[WordIdentifier] The `WordIdentifier` instance used by the DTA models. Raises ------ ValueError If vocabulary size besides 26 and 32000 is passed, a `ValueError` is raised. \"\"\" if vocab_size not in [ 26 , 32000 ]: raise ValueError ( \"Supported vocab sizes are 26 and 32000\" ) protein_vocab_path = f \" { package_path } /data/word_identification/protein\" vocab_path = f \" { protein_vocab_path } /uniprot_26.json\" if vocab_size == 32000 : vocab_path = f \" { protein_vocab_path } /uniprot_bpe_32000.json\" return WordIdentifier . from_file ( vocab_path )","title":"load_protein_word_identifier()"},{"location":"api/utils/","text":"utils load_json ( path ) Loads a json file into a dictionary. Parameters: Name Type Description Default path str Path to the .json file to load. required Returns: Type Description Dict Content of the .json file as a dictionary. Source code in pydebiaseddta\\utils.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_json ( path : str ) -> Dict : \"\"\"Loads a json file into a dictionary. Parameters ---------- path : str Path to the .json file to load. Returns ------- Dict Content of the .json file as a dictionary. \"\"\" with open ( path , \"r\" ) as f : return json . load ( f ) load_sample_dta_data ( mini = False ) Loads a portion of the BDB dataset for fast experimenting. Parameters: Name Type Description Default mini bool , optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to True for fast prototyping and False to quickly train a model. Defaults to False . False Returns: Type Description Dict [ str , List ] The dictionary has three keys: \"train\" , \"val\" , and \"test\" , each corresponding to different folds of the dataset. Each key maps to a list with three elements: list of chemicals , list of proteins , and list of affinity scores . The elements in the same index of the lists correspond to a drug-target affinity measurement. Source code in pydebiaseddta\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_sample_dta_data ( mini : bool = False ) -> Dict [ str , List ]: \"\"\"Loads a portion of [the BDB dataset](https://arxiv.org/pdf/2107.05556.pdf) for fast experimenting. Parameters ---------- mini : bool, optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to `True` for fast prototyping and `False` to quickly train a model. Defaults to `False`. Returns ------- Dict[str, List] The dictionary has three keys: `\"train\"`, `\"val\"`, and `\"test\"`, each corresponding to different folds of the dataset. Each key maps to a list with three elements: *list of chemicals*, *list of proteins*, and *list of affinity scores*. The elements in the same index of the lists correspond to a drug-target affinity measurement. \"\"\" sample_data_path = f \" { package_path } /data/dta_sample_data/dta_sample_data.json\" if mini : sample_data_path = f \" { package_path } /data/dta/dta_sample_data.mini.json\" with open ( sample_data_path ) as f : return json . load ( f ) load_sample_smiles () Returns examples SMILES strings from ChEMBL for testing. Returns: Type Description List [ str ] SMILES examples from ChEMBL. Source code in pydebiaseddta\\utils.py 30 31 32 33 34 35 36 37 38 39 40 def load_sample_smiles () -> List [ str ]: \"\"\"Returns examples SMILES strings from ChEMBL for testing. Returns ------- List[str] SMILES examples from ChEMBL. \"\"\" sample_data_path = f \" { package_path } /data/sequence/chembl27.mini.smiles\" with open ( sample_data_path ) as f : return [ line . strip () for line in f . readlines ()] save_json ( obj , path ) Saves a dictionary in json format. The indent is set to 4 for readability. Parameters: Name Type Description Default obj Dict Dictionary to store. required path str Path to store the .json file. required Returns: Type Description None Source code in pydebiaseddta\\utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json ( obj : Dict , path : str ) -> None : \"\"\"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters ---------- obj : Dict Dictionary to store. path : str Path to store the .json file. Returns ------- None \"\"\" with open ( path , \"w\" ) as f : json . dump ( obj , f , indent = 4 )","title":"utils"},{"location":"api/utils/#utils","text":"","title":"utils"},{"location":"api/utils/#pydebiaseddta.utils.load_json","text":"Loads a json file into a dictionary. Parameters: Name Type Description Default path str Path to the .json file to load. required Returns: Type Description Dict Content of the .json file as a dictionary. Source code in pydebiaseddta\\utils.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def load_json ( path : str ) -> Dict : \"\"\"Loads a json file into a dictionary. Parameters ---------- path : str Path to the .json file to load. Returns ------- Dict Content of the .json file as a dictionary. \"\"\" with open ( path , \"r\" ) as f : return json . load ( f )","title":"load_json()"},{"location":"api/utils/#pydebiaseddta.utils.load_sample_dta_data","text":"Loads a portion of the BDB dataset for fast experimenting. Parameters: Name Type Description Default mini bool , optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to True for fast prototyping and False to quickly train a model. Defaults to False . False Returns: Type Description Dict [ str , List ] The dictionary has three keys: \"train\" , \"val\" , and \"test\" , each corresponding to different folds of the dataset. Each key maps to a list with three elements: list of chemicals , list of proteins , and list of affinity scores . The elements in the same index of the lists correspond to a drug-target affinity measurement. Source code in pydebiaseddta\\utils.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_sample_dta_data ( mini : bool = False ) -> Dict [ str , List ]: \"\"\"Loads a portion of [the BDB dataset](https://arxiv.org/pdf/2107.05556.pdf) for fast experimenting. Parameters ---------- mini : bool, optional Whether to load all drug-target pairs embedded in the library, or a mini version. Set to `True` for fast prototyping and `False` to quickly train a model. Defaults to `False`. Returns ------- Dict[str, List] The dictionary has three keys: `\"train\"`, `\"val\"`, and `\"test\"`, each corresponding to different folds of the dataset. Each key maps to a list with three elements: *list of chemicals*, *list of proteins*, and *list of affinity scores*. The elements in the same index of the lists correspond to a drug-target affinity measurement. \"\"\" sample_data_path = f \" { package_path } /data/dta_sample_data/dta_sample_data.json\" if mini : sample_data_path = f \" { package_path } /data/dta/dta_sample_data.mini.json\" with open ( sample_data_path ) as f : return json . load ( f )","title":"load_sample_dta_data()"},{"location":"api/utils/#pydebiaseddta.utils.load_sample_smiles","text":"Returns examples SMILES strings from ChEMBL for testing. Returns: Type Description List [ str ] SMILES examples from ChEMBL. Source code in pydebiaseddta\\utils.py 30 31 32 33 34 35 36 37 38 39 40 def load_sample_smiles () -> List [ str ]: \"\"\"Returns examples SMILES strings from ChEMBL for testing. Returns ------- List[str] SMILES examples from ChEMBL. \"\"\" sample_data_path = f \" { package_path } /data/sequence/chembl27.mini.smiles\" with open ( sample_data_path ) as f : return [ line . strip () for line in f . readlines ()]","title":"load_sample_smiles()"},{"location":"api/utils/#pydebiaseddta.utils.save_json","text":"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters: Name Type Description Default obj Dict Dictionary to store. required path str Path to store the .json file. required Returns: Type Description None Source code in pydebiaseddta\\utils.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json ( obj : Dict , path : str ) -> None : \"\"\"Saves a dictionary in json format. The indent is set to 4 for readability. Parameters ---------- obj : Dict Dictionary to store. path : str Path to store the .json file. Returns ------- None \"\"\" with open ( path , \"w\" ) as f : json . dump ( obj , f , indent = 4 )","title":"save_json()"}]}